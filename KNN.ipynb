{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)]\n",
      "scipy: 1.2.1\n",
      "numpy: 1.16.2\n",
      "matplotlib: 3.0.3\n",
      "pandas: 0.24.2\n",
      "sklearn: 0.20.3\n",
      "numba: 0.43.0\n",
      "pyod: 0.6.8\n"
     ]
    }
   ],
   "source": [
    "# Check the versions of libraries\n",
    " \n",
    "# Python version\n",
    "import sys\n",
    "print('Python: {}'.format(sys.version))\n",
    "# scipy\n",
    "import scipy\n",
    "print('scipy: {}'.format(scipy.__version__))\n",
    "# numpy\n",
    "import numpy \n",
    "print('numpy: {}'.format(numpy.__version__))\n",
    "# matplotlib\n",
    "import matplotlib\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "# pandas\n",
    "import pandas \n",
    "print('pandas: {}'.format(pandas.__version__))\n",
    "# scikit-learn\n",
    "import sklearn\n",
    "print('sklearn: {}'.format(sklearn.__version__))\n",
    "# numba\n",
    "import numba\n",
    "print('numba: {}'.format(numba.__version__))\n",
    "# pyod\n",
    "from pyod import version\n",
    "print('pyod: {}'.format(version.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from numpy.linalg import inv, det\n",
    "import pandas as pd \n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "from tensorflow.python.keras.layers.core import Dense, Activation, Dropout\n",
    "from tensorflow.python.keras.layers.recurrent import LSTM\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "from keras import backend as K\n",
    "import time\n",
    "import gc\n",
    "\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "from pyod.models.knn import KNN   # kNN detector\n",
    "from pyod.utils.utility import standardizer\n",
    "\n",
    "\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.feature_bagging import FeatureBagging\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.mcd import MCD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models.pca import PCA\n",
    "from pyod.models.lscp import LSCP\n",
    "from pyod.utils.utility import standardizer\n",
    "from pyod.utils.utility import precision_n_scores\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings('once')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "#0 = all messages are logged (default behavior)\n",
    "#1 = INFO messages are not printed\n",
    "#2 = INFO and WARNING messages are not printed\n",
    "#3 = INFO, WARNING, and ERROR messages are not printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dfs = pd.read_csv(r'FinalProcessing_out.csv', header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of observations:\",len(dfs))\n",
    "print(\"Number of normal transactions:\",sum(dfs.cia==0))\n",
    "print(\"Number of outliers:\",sum(dfs.cia==1))\n",
    "print(\"Ratio of outliers versus normal transaction:\",sum(dfs.cia==1)/sum(dfs.cia==0))\n",
    "print(\"Dataset Shape:\",dfs.shape)\n",
    "dfs.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    x = dfs.drop(columns = [\"cia\"])\n",
    "    y = dfs.cia\n",
    "\n",
    "    random_state = np.random.RandomState(40)\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    \n",
    "    # Separatig data for training and for testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=random_state)\n",
    "    \n",
    "    # standardizing data for processing\n",
    "    X_train_norm, X_test_norm = standardizer(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUPERVISED LEARNING\n",
    "# train kNN detector\n",
    "\n",
    "Knn = []\n",
    "Precision = []\n",
    "Recall = []\n",
    "F1score = []\n",
    "\n",
    "for k in range (3, 4):\n",
    "    # instantiate learning model \n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    #knn = KNeighborsClassifier(n_neighbors=k,algorithm='kd_tree')\n",
    "    # fitting the model\n",
    "    knn.fit(X_train, y_train)\n",
    "    # predict the response\n",
    "    pred = knn.predict(X_test)\n",
    "    \n",
    "   \n",
    "    #print(y_test)\n",
    "    #print( pred)\n",
    "    # evaluate accuracy\n",
    "    print (\"===========================================\")\n",
    "    print (\"FOR Knn_Neighbors =\", k)\n",
    "    #print (\"Accuracy Score for \",accuracy_score(y_test, pred))\n",
    "    #print(\"Precision Score\",round(metrics.precision_score(y_test, pred,average='micro'), ndigits=5))\n",
    "    #print(\"Recall Score\",round(metrics.recall_score(y_test, pred,average='micro'), ndigits=5))\n",
    "    #print(\"F1 Score\",round(metrics.f1_score(y_test, pred,average='micro'), ndigits=5))\n",
    "\n",
    "\n",
    "    #print(\"Micro\")\n",
    "    #print(\"Precision Score\",round(metrics.precision_score(y_test, pred.round(),average='micro'), ndigits=5))\n",
    "    #print(\"Recall Score\",round(metrics.recall_score(y_test, pred.round(),average='micro'), ndigits=5))\n",
    "    #print(\"F1 Score\",round(metrics.f1_score(y_test, pred.round(),average='micro'), ndigits=5))\n",
    "\n",
    "    print(\"Macro\")\n",
    "    print(\"Precision Score\",round(metrics.precision_score(y_test, pred.round(),average='macro'), ndigits=5))\n",
    "    print(\"Recall Score\",round(metrics.recall_score(y_test, pred.round(),average='macro'), ndigits=5))\n",
    "    print(\"F1 Score\",round(metrics.f1_score(y_test, pred.round(),average='macro'), ndigits=5))\n",
    "\n",
    "    Knn.append(k)\n",
    "    Precision.append(round(metrics.precision_score(y_test, pred.round(),average='macro'), ndigits=5))\n",
    "    Recall .append(round(metrics.recall_score(y_test, pred.round(),average='macro'), ndigits=5))\n",
    "    F1score.append(round(metrics.f1_score(y_test, pred.round(),average='macro'), ndigits=5))\n",
    "    #print(\"Weighted\")\n",
    "    #print(\"Precision Score\",round(metrics.precision_score(y_test, pred.round(),average='weighted'), ndigits=5))\n",
    "    #print(\"Recall Score\",round(metrics.recall_score(y_test, pred.round(),average='weighted'), ndigits=5))\n",
    "    #print(\"F1 Score\",round(metrics.f1_score(y_test, pred.round(),average='weighted'), ndigits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Knn,F1score)\n",
    "plt.plot(Knn,Recall)\n",
    "plt.plot(Knn,Precision)\n",
    "#plt.plot(F1score)\n",
    "plt.title('KNN - F1_score vs #Neighbors')\n",
    "plt.ylabel('F1_score')\n",
    "plt.xlabel('# KNN')\n",
    "plt.legend(['F1_score', 'Recall','Precision'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.multiclass import unique_labels\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    \n",
    "\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           xlabel='Actual Values',\n",
    "           ylabel='Predicted Values')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_labels(y_test, pred))\n",
    "np.set_printoptions(precision=2)\n",
    "class_names = [ \"Vulnerability\",\"Normal\"]#y_test.target_names\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix( pred, y_test,classes=class_names, title='Confusion matrix')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(pred,y_test, classes=unique_labels(y_test, pred), normalize=True, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"Macro\")\n",
    "    print(\"Precision Score\",round(metrics.precision_score(y_test, pred.round(),average='macro'), ndigits=5))\n",
    "    print(\"Recall Score\",round(metrics.recall_score(y_test, pred.round(),average='macro'), ndigits=5))\n",
    "    print(\"F1 Score\",round(metrics.f1_score(y_test, pred.round(),average='macro'), ndigits=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
