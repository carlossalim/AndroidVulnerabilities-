{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)]\n",
      "scipy: 1.2.1\n",
      "numpy: 1.16.2\n",
      "matplotlib: 3.0.3\n",
      "pandas: 0.24.2\n",
      "sklearn: 0.20.3\n",
      "numba: 0.43.0\n",
      "pyod: 0.6.8\n"
     ]
    }
   ],
   "source": [
    "# Check the versions of libraries\n",
    " \n",
    "# Python version\n",
    "import sys\n",
    "print('Python: {}'.format(sys.version))\n",
    "# scipy\n",
    "import scipy\n",
    "print('scipy: {}'.format(scipy.__version__))\n",
    "# numpy\n",
    "import numpy \n",
    "print('numpy: {}'.format(numpy.__version__))\n",
    "# matplotlib\n",
    "import matplotlib\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "# pandas\n",
    "import pandas \n",
    "print('pandas: {}'.format(pandas.__version__))\n",
    "# scikit-learn\n",
    "import sklearn\n",
    "print('sklearn: {}'.format(sklearn.__version__))\n",
    "# numba\n",
    "import numba\n",
    "print('numba: {}'.format(numba.__version__))\n",
    "# pyod\n",
    "from pyod import version\n",
    "print('pyod: {}'.format(version.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from numpy.linalg import inv, det\n",
    "import pandas as pd \n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.python.keras.layers.core import Dense, Activation, Dropout\n",
    "from tensorflow.python.keras.layers.recurrent import LSTM\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from numpy import arange, sin, pi, random\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "#  USING ABOVE\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "from pyod.models.knn import KNN   # kNN detector\n",
    "from pyod.utils.utility import standardizer\n",
    "\n",
    "\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.feature_bagging import FeatureBagging\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.mcd import MCD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models.pca import PCA\n",
    "from pyod.models.lscp import LSCP\n",
    "from pyod.utils.utility import standardizer\n",
    "from pyod.utils.utility import precision_n_scores\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings('once')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "#0 = all messages are logged (default behavior)\n",
    "#1 = INFO messages are not printed\n",
    "#2 = INFO and WARNING messages are not printed\n",
    "#3 = INFO, WARNING, and ERROR messages are not printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations: 6465\n",
      "Number of normal transactions: 6225\n",
      "Number of outliers: 240\n",
      "Ratio of outliers versus normal transaction: 0.03855421686746988\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cia</th>\n",
       "      <th>_exit</th>\n",
       "      <th>capset</th>\n",
       "      <th>clock_gettime</th>\n",
       "      <th>close</th>\n",
       "      <th>connect</th>\n",
       "      <th>epoll_ctl</th>\n",
       "      <th>faccessat</th>\n",
       "      <th>fchmodat</th>\n",
       "      <th>fstatat64</th>\n",
       "      <th>...</th>\n",
       "      <th>pwrite64</th>\n",
       "      <th>renameat</th>\n",
       "      <th>rt_sigprocmask</th>\n",
       "      <th>rt_sigtimedwait</th>\n",
       "      <th>sendmsg</th>\n",
       "      <th>set_tid_address</th>\n",
       "      <th>setpriority</th>\n",
       "      <th>sigaction</th>\n",
       "      <th>sigaltstack</th>\n",
       "      <th>writev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cia  _exit  capset  clock_gettime  close  connect  epoll_ctl  faccessat  \\\n",
       "0    0    0.0     0.0           14.0   10.0      0.0        0.0        0.0   \n",
       "1    0    1.0     0.0            2.0    0.0      0.0        0.0        0.0   \n",
       "2    0    1.0     0.0            2.0    0.0      0.0        0.0        0.0   \n",
       "3    0    1.0     0.0            0.0    0.0      0.0        0.0        0.0   \n",
       "4    0    1.0     0.0            0.0    0.0      0.0        0.0        0.0   \n",
       "\n",
       "   fchmodat  fstatat64  ...  pwrite64  renameat  rt_sigprocmask  \\\n",
       "0       0.0        0.0  ...       0.0       0.0             0.0   \n",
       "1       0.0        0.0  ...       0.0       0.0             0.0   \n",
       "2       0.0        0.0  ...       0.0       0.0             0.0   \n",
       "3       0.0        0.0  ...       0.0       0.0             0.0   \n",
       "4       0.0        0.0  ...       0.0       0.0             0.0   \n",
       "\n",
       "   rt_sigtimedwait  sendmsg  set_tid_address  setpriority  sigaction  \\\n",
       "0              0.0      0.0              0.0          0.0        0.0   \n",
       "1              0.0      0.0              0.0          0.0        0.0   \n",
       "2              0.0      0.0              0.0          0.0        0.0   \n",
       "3              0.0      0.0              0.0          0.0        0.0   \n",
       "4              0.0      0.0              0.0          0.0        0.0   \n",
       "\n",
       "   sigaltstack  writev  \n",
       "0          0.0     0.0  \n",
       "1          0.0     0.0  \n",
       "2          0.0     0.0  \n",
       "3          0.0     0.0  \n",
       "4          0.0     0.0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "dfs = pd.read_csv(r'FinalProcessing_out.csv', header = 0)\n",
    "#################################################\n",
    "#scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "#dfs = scaler.fit_transform(dfs)\n",
    "################################################\n",
    "\n",
    "\n",
    "#columns = np.full((corr.shape[0],), False, dtype=bool)\n",
    "#corr = dfs.corr()\n",
    "#selected_columns = dfs.columns[columns]\n",
    "#columns[7] = True\n",
    "#columns[0] = True\n",
    "#columns[2] = True\n",
    "#print(columns)\n",
    "#print(selected_columns  )\n",
    "#selected_columns = dfs.columns[columns]\n",
    "#print(selected_columns  )\n",
    "#dfs = dfs[selected_columns]\n",
    "print(\"Number of observations:\",len(dfs))\n",
    "print(\"Number of normal transactions:\",sum(dfs.cia==0))\n",
    "print(\"Number of outliers:\",sum(dfs.cia==1))\n",
    "print(\"Ratio of outliers versus normal transaction:\",sum(dfs.cia==1)/sum(dfs.cia==0))\n",
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dfs.cia.astype(float)\n",
    "dfs = dfs.drop(columns = [\"cia\"])#@@@.astype(float)\n",
    "#dfs = dfs.drop(columns = [\"timestamp\"])#@@@.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6465, 36)\n"
     ]
    }
   ],
   "source": [
    "print(dfs.shape)\n",
    "#print(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "#scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dfs = scaler.fit_transform(dfs)\n",
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outliers_percentage:  3.7123 %\n",
      "(6465, 36)\n",
      "(6465,)\n",
      "(3232, 1, 36)\n",
      "(3233, 1, 36)\n",
      "(3232,)\n",
      "(3233,)\n",
      "Number of train outliers: 117.0\n",
      "Number of test outliers: 123.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing  \n",
    "#Without type float gives a warning\n",
    "x = dfs\n",
    "#//CENTERING DATA\n",
    "#x = preprocessing.scale(x, with_mean='True', with_std='False')\n",
    "\n",
    "#y = dfs.cia#@@@.astype(float)\n",
    "#print(y)\n",
    "outliers_fraction = np.count_nonzero(y) / len(y)\n",
    "outliers_percentage = round(outliers_fraction * 100, ndigits=4)\n",
    "\n",
    "#print(outliers_fraction)\n",
    "print(\"outliers_percentage: \",outliers_percentage,\"%\")\n",
    "\n",
    "\n",
    "# Define nine outlier detection tools to be compared\n",
    "#RandomState - train_test_split splits arrays or matrices into random train and test subsets. \n",
    "# That means that everytime you run it without specifying random_state, you will get a different result, this is expected behavior.\n",
    "# On the other hand if you use random_state=some_number, then you can guarantee that the output of Run 1 will be equal to the output of Run 2,\n",
    "random_state = np.random.RandomState(42)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "# Spliting for training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.5,  random_state=random_state)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=False, random_state=random_state)\n",
    "\n",
    "\n",
    "######################################################\n",
    "numpy.savetxt(\"X_test.csv\", X_test, delimiter=\",\")\n",
    "numpy.savetxt(\"X_train.csv\", X_train, delimiter=\",\")\n",
    "#######################################################\n",
    "\n",
    "\n",
    "# standardizing data for processing\n",
    "X_train, X_test = standardizer(X_train, X_test)\n",
    "\n",
    "######################################################\n",
    "numpy.savetxt(\"X_test2.csv\", X_test, delimiter=\",\")\n",
    "numpy.savetxt(\"X_train2.csv\", X_train, delimiter=\",\")\n",
    "#######################################################\n",
    "\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "data = np.array(X_train, dtype=float)\n",
    "X_train = np.reshape(data, (data.shape[0],1,data.shape[1]))\n",
    "data = np.array(X_test, dtype=float)\n",
    "X_test = np.reshape(data, (data.shape[0],1,data.shape[1]))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(\"Number of train outliers:\",sum(y_train))\n",
    "print(\"Number of test outliers:\",sum(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    print(\"Buildibg Model...\")\n",
    "    layers = {'input': 1, 'hidden1':64, 'hidden2': 64, 'hidden3': 64, 'output': 1}\n",
    "\n",
    "    look_back = data.shape[1]\n",
    "    model.add(LSTM(4, input_shape=(1, look_back), return_sequences=True))\n",
    "       \n",
    "    #Dropout is a regularization method where input and recurrent connections to LSTM units are probabilistically excluded from \n",
    "    #activation and weight updates while training a network. This has the effect of reducing overfitting \n",
    "    #and improving model performance.\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(layers['hidden1'], return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(layers['hidden2'], return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "   \n",
    "    model.add(LSTM(layers['hidden3'], return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    start = time.time()\n",
    "    #model1.compile(loss=\"mse\", optimizer=\"rmsprop\")\n",
    "    print (\"Compilation Time : \", time.time() - start)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3232, 1, 36) (3232,) (3233, 1, 36) (3233,)\n",
      "\n",
      "Data Loaded. Compiling...\n",
      "\n",
      "Buildibg Model...\n",
      "WARNING:tensorflow:From D:\\Users\\carlo\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\Users\\carlo\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\Users\\carlo\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Compilation Time :  0.0\n",
      "Buildibg Model...\n",
      "Training...\n",
      "Train on 3232 samples, validate on 3233 samples\n",
      "WARNING:tensorflow:From D:\\Users\\carlo\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1000\n",
      "3232/3232 [==============================] - 76s 23ms/sample - loss: 0.0377 - val_loss: 0.0336\n",
      "Epoch 2/1000\n",
      "3232/3232 [==============================] - 76s 24ms/sample - loss: 0.0312 - val_loss: 0.0320\n",
      "Epoch 3/1000\n",
      "3232/3232 [==============================] - 72s 22ms/sample - loss: 0.0298 - val_loss: 0.0311\n",
      "Epoch 4/1000\n",
      "3232/3232 [==============================] - 72s 22ms/sample - loss: 0.0292 - val_loss: 0.0317\n",
      "Epoch 5/1000\n",
      "3232/3232 [==============================] - 72s 22ms/sample - loss: 0.0301 - val_loss: 0.0309\n",
      "Epoch 6/1000\n",
      "3232/3232 [==============================] - 72s 22ms/sample - loss: 0.0294 - val_loss: 0.0308\n",
      "Epoch 7/1000\n",
      "3232/3232 [==============================] - 72s 22ms/sample - loss: 0.0290 - val_loss: 0.0296\n",
      "Epoch 8/1000\n",
      "3232/3232 [==============================] - 71s 22ms/sample - loss: 0.0285 - val_loss: 0.0296\n",
      "Epoch 9/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0279 - val_loss: 0.0300\n",
      "Epoch 10/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0276 - val_loss: 0.0314\n",
      "Epoch 11/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0281 - val_loss: 0.0321\n",
      "Epoch 12/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0281 - val_loss: 0.0298\n",
      "Epoch 13/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0286 - val_loss: 0.0301\n",
      "Epoch 14/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0277 - val_loss: 0.0286\n",
      "Epoch 15/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0281 - val_loss: 0.0303\n",
      "Epoch 16/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0277 - val_loss: 0.0284\n",
      "Epoch 17/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0276 - val_loss: 0.0290\n",
      "Epoch 18/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0282 - val_loss: 0.0289\n",
      "Epoch 19/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0277 - val_loss: 0.0290\n",
      "Epoch 20/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0284 - val_loss: 0.0293\n",
      "Epoch 21/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0269 - val_loss: 0.0323\n",
      "Epoch 22/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0270 - val_loss: 0.0286\n",
      "Epoch 23/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0269 - val_loss: 0.0285\n",
      "Epoch 24/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0273 - val_loss: 0.0289\n",
      "Epoch 25/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0276 - val_loss: 0.0281\n",
      "Epoch 26/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0263 - val_loss: 0.0277\n",
      "Epoch 27/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0271 - val_loss: 0.0291\n",
      "Epoch 28/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0277 - val_loss: 0.0275\n",
      "Epoch 29/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0266 - val_loss: 0.0281\n",
      "Epoch 30/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0265 - val_loss: 0.0282\n",
      "Epoch 31/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0261 - val_loss: 0.0283\n",
      "Epoch 32/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0265 - val_loss: 0.0283\n",
      "Epoch 33/1000\n",
      "3232/3232 [==============================] - 71s 22ms/sample - loss: 0.0263 - val_loss: 0.0278\n",
      "Epoch 34/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0268 - val_loss: 0.0295\n",
      "Epoch 35/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0270 - val_loss: 0.0284\n",
      "Epoch 36/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0262 - val_loss: 0.0284\n",
      "Epoch 37/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0268 - val_loss: 0.0291\n",
      "Epoch 38/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0259 - val_loss: 0.0291 l - ET\n",
      "Epoch 39/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0266 - val_loss: 0.0281\n",
      "Epoch 40/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0266 - val_loss: 0.0285\n",
      "Epoch 41/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0261 - val_loss: 0.02740. - ETA: 8 - ETA: 7s - loss\n",
      "Epoch 42/1000\n",
      "3232/3232 [==============================] - 71s 22ms/sample - loss: 0.0262 - val_loss: 0.0322\n",
      "Epoch 43/1000\n",
      "3232/3232 [==============================] - 72s 22ms/sample - loss: 0.0273 - val_loss: 0.0296\n",
      "Epoch 44/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0256 - val_loss: 0.0286\n",
      "Epoch 45/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0255 - val_loss: 0.0301\n",
      "Epoch 46/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0244 - val_loss: 0.0276\n",
      "Epoch 47/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0258 - val_loss: 0.0298 - ETA: 4s - loss: 0.0 - ETA: 4s - loss: 0.026 - ETA: 4s - loss: 0. - ET -\n",
      "Epoch 48/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0258 - val_loss: 0.0299\n",
      "Epoch 49/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0258 - val_loss: 0.0278\n",
      "Epoch 50/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0260 - val_loss: 0.0286\n",
      "Epoch 51/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0253 - val_loss: 0.0310\n",
      "Epoch 52/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0265 - val_loss: 0.0285\n",
      "Epoch 53/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0252 - val_loss: 0.0287\n",
      "Epoch 54/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0258 - val_loss: 0.0283\n",
      "Epoch 55/1000\n",
      "3232/3232 [==============================] - 71s 22ms/sample - loss: 0.0257 - val_loss: 0.0297\n",
      "Epoch 56/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0254 - val_loss: 0.0277\n",
      "Epoch 57/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0251 - val_loss: 0.0297\n",
      "Epoch 58/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0263 - val_loss: 0.0282\n",
      "Epoch 59/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0259 - val_loss: 0.0288\n",
      "Epoch 60/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0264 - val_loss: 0.0291\n",
      "Epoch 61/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0255 - val_loss: 0.0290\n",
      "Epoch 62/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0261 - val_loss: 0.0283\n",
      "Epoch 63/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0261 - val_loss: 0.0300\n",
      "Epoch 64/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0252 - val_loss: 0.0296\n",
      "Epoch 65/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0250 - val_loss: 0.0283: 1s - loss: 0 - ETA: 0s - loss: 0.025\n",
      "Epoch 66/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0253 - val_loss: 0.0285\n",
      "Epoch 67/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0247 - val_loss: 0.0290 - lo - ETA: 1s - los -\n",
      "Epoch 68/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0258 - val_loss: 0.0283\n",
      "Epoch 69/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0251 - val_loss: 0.0286\n",
      "Epoch 70/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0252 - val_loss: 0.0281ETA: 0s - loss: \n",
      "Epoch 71/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0246 - val_loss: 0.0278\n",
      "Epoch 72/1000\n",
      "3232/3232 [==============================] - 71s 22ms/sample - loss: 0.0254 - val_loss: 0.0283\n",
      "Epoch 73/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0244 - val_loss: 0.0281\n",
      "Epoch 74/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0251 - val_loss: 0.0297\n",
      "Epoch 75/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0252 - val_loss: 0.0279\n",
      "Epoch 76/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0246 - val_loss: 0.0288\n",
      "Epoch 77/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0252 - val_loss: 0.0278s - ETA: 0s - \n",
      "Epoch 78/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0247 - val_loss: 0.0275\n",
      "Epoch 79/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0249 - val_loss: 0.0280\n",
      "Epoch 80/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0245 - val_loss: 0.0282\n",
      "Epoch 81/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0242 - val_loss: 0.0307- l  - ET - ETA: 2s - los  - ETA: 0s - loss\n",
      "Epoch 82/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0251 - val_loss: 0.0279\n",
      "Epoch 83/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0249 - val_loss: 0.0280\n",
      "Epoch 84/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0243 - val_loss: 0.0326\n",
      "Epoch 85/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0275 - val_loss: 0.0275TA:  - ETA: 1s - loss: 0.0 -\n",
      "Epoch 86/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0248 - val_loss: 0.0276\n",
      "Epoch 87/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0253 - val_loss: 0.0290\n",
      "Epoch 88/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0245 - val_loss: 0.0322ss: 0. - ETA:  - ETA: 0s - loss:\n",
      "Epoch 89/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0246 - val_loss: 0.0282\n",
      "Epoch 90/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0248 - val_loss: 0.0286.02 - ETA: 0s - loss\n",
      "Epoch 91/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0251 - val_loss: 0.0276\n",
      "Epoch 92/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0248 - val_loss: 0.0280 ETA: \n",
      "Epoch 93/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0245 - val_loss: 0.0277\n",
      "Epoch 94/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0247 - val_loss: 0.0272\n",
      "Epoch 95/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0233 - val_loss: 0.02789 - ETA: 8s - ETA: 0s - loss\n",
      "Epoch 96/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0242 - val_loss: 0.0277\n",
      "Epoch 97/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0243 - val_loss: 0.0283\n",
      "Epoch 98/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0245 - val_loss: 0.0278\n",
      "Epoch 99/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0240 - val_loss: 0.0275s\n",
      "Epoch 100/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0241 - val_loss: 0.0289\n",
      "Epoch 101/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0240 - val_loss: 0.0280\n",
      "Epoch 102/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0245 - val_loss: 0.0271\n",
      "Epoch 103/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0246 - val_loss: 0.0282\n",
      "Epoch 104/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0243 - val_loss: 0.0301s - lo\n",
      "Epoch 105/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0226 - val_loss: 0.0281ET - ETA: 4s - loss: 0 - ET\n",
      "Epoch 106/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0238 - val_loss: 0.0281\n",
      "Epoch 107/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0243 - val_loss: 0.0270\n",
      "Epoch 108/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0250 - val_loss: 0.0288\n",
      "Epoch 109/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0240 - val_loss: 0.0272\n",
      "Epoch 110/1000\n",
      "3232/3232 [==============================] - 71s 22ms/sample - loss: 0.0247 - val_loss: 0.0280\n",
      "Epoch 111/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0244 - val_loss: 0.0275\n",
      "Epoch 112/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0239 - val_loss: 0.0271\n",
      "Epoch 113/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0244 - val_loss: 0.0316\n",
      "Epoch 114/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0243 - val_loss: 0.0279TA: 2s - loss:  - ET - ETA: 0s - loss: 0.02 - ETA: 0s - lo\n",
      "Epoch 115/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0245 - val_loss: 0.0296\n",
      "Epoch 116/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0246 - val_loss: 0.0351\n",
      "Epoch 117/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0238 - val_loss: 0.0273\n",
      "Epoch 118/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0240 - val_loss: 0.0275\n",
      "Epoch 119/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0254 - val_loss: 0.0284\n",
      "Epoch 120/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0253 - val_loss: 0.0274\n",
      "Epoch 121/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0235 - val_loss: 0.0286\n",
      "Epoch 122/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0244 - val_loss: 0.0290\n",
      "Epoch 123/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0243 - val_loss: 0.0302 ETA:  - ETA: 0s - loss: 0.\n",
      "Epoch 124/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0249 - val_loss: 0.0293\n",
      "Epoch 125/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0240 - val_loss: 0.0266\n",
      "Epoch 126/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0245 - val_loss: 0.0286\n",
      "Epoch 127/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0239 - val_loss: 0.0278\n",
      "Epoch 128/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0243 - val_loss: 0.0269:  \n",
      "Epoch 129/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0243 - val_loss: 0.0279\n",
      "Epoch 130/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0227 - val_loss: 0.0274\n",
      "Epoch 131/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0238 - val_loss: 0.0290\n",
      "Epoch 132/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0236 - val_loss: 0.0303\n",
      "Epoch 133/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0249 - val_loss: 0.0261\n",
      "Epoch 134/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0243 - val_loss: 0.0275\n",
      "Epoch 135/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0241 - val_loss: 0.0268\n",
      "Epoch 136/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0241 - val_loss: 0.0280 loss: 0 - ETA: 0s - loss: 0.024\n",
      "Epoch 137/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0243 - val_loss: 0.0267ss - ETA: 0s - loss:\n",
      "Epoch 138/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0243 - val_loss: 0.0268\n",
      "Epoch 139/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0237 - val_loss: 0.0271\n",
      "Epoch 140/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0241 - val_loss: 0.0277\n",
      "Epoch 141/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0241 - val_loss: 0.0287\n",
      "Epoch 142/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0236 - val_loss: 0.0280\n",
      "Epoch 143/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0246 - val_loss: 0.0268\n",
      "Epoch 144/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0252 - val_loss: 0.0268\n",
      "Epoch 145/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0235 - val_loss: 0.0267\n",
      "Epoch 146/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0237 - val_loss: 0.0272\n",
      "Epoch 147/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0235 - val_loss: 0.0263\n",
      "Epoch 148/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0242 - val_loss: 0.0280\n",
      "Epoch 149/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0239 - val_loss: 0.0270\n",
      "Epoch 150/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0241 - val_loss: 0.0270\n",
      "Epoch 151/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0239 - val_loss: 0.0288\n",
      "Epoch 152/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0237 - val_loss: 0.0290A: \n",
      "Epoch 153/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0244 - val_loss: 0.0267 1s -  - ETA: \n",
      "Epoch 154/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0233 - val_loss: 0.0287\n",
      "Epoch 155/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0245 - val_loss: 0.0273\n",
      "Epoch 156/1000\n",
      "3232/3232 [==============================] - 69s 22ms/sample - loss: 0.0246 - val_loss: 0.027113s  - ETA: 10s -  - ETA: 9s - los - ETA: 9s - l\n",
      "Epoch 157/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0249 - val_loss: 0.0273\n",
      "Epoch 158/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0236 - val_loss: 0.0273\n",
      "Epoch 159/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0242 - val_loss: 0.0273\n",
      "Epoch 160/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0238 - val_loss: 0.0333\n",
      "Epoch 161/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0268 - val_loss: 0.0280\n",
      "Epoch 162/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0241 - val_loss: 0.0283\n",
      "Epoch 163/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0246 - val_loss: 0.0291\n",
      "Epoch 164/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0227 - val_loss: 0.0275\n",
      "Epoch 165/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0240 - val_loss: 0.0289\n",
      "Epoch 166/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0237 - val_loss: 0.0278\n",
      "Epoch 167/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0244 - val_loss: 0.0276\n",
      "Epoch 168/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0247 - val_loss: 0.0262 - loss:\n",
      "Epoch 169/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0240 - val_loss: 0.0262\n",
      "Epoch 170/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0228 - val_loss: 0.0266- ETA: 1s - loss:  - E\n",
      "Epoch 171/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0245 - val_loss: 0.0274 0s - \n",
      "Epoch 172/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0244 - val_loss: 0.0278\n",
      "Epoch 173/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0233 - val_loss: 0.0279\n",
      "Epoch 174/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0243 - val_loss: 0.0264\n",
      "Epoch 175/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0240 - val_loss: 0.0284\n",
      "Epoch 176/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0231 - val_loss: 0.0264\n",
      "Epoch 177/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0236 - val_loss: 0.0280\n",
      "Epoch 178/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0243 - val_loss: 0.0271\n",
      "Epoch 179/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0239 - val_loss: 0.0270\n",
      "Epoch 180/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0238 - val_loss: 0.02511 - ETA: 0s - lo\n",
      "Epoch 181/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0233 - val_loss: 0.0274\n",
      "Epoch 182/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0241 - val_loss: 0.0267\n",
      "Epoch 183/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0234 - val_loss: 0.0277\n",
      "Epoch 184/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0242 - val_loss: 0.0267\n",
      "Epoch 185/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0226 - val_loss: 0.0281\n",
      "Epoch 186/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0234 - val_loss: 0.0269\n",
      "Epoch 187/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0240 - val_loss: 0.0260\n",
      "Epoch 188/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0237 - val_loss: 0.0268\n",
      "Epoch 189/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0232 - val_loss: 0.0282\n",
      "Epoch 190/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0239 - val_loss: 0.0266\n",
      "Epoch 191/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0236 - val_loss: 0.0267\n",
      "Epoch 192/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0235 - val_loss: 0.0264\n",
      "Epoch 193/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0243 - val_loss: 0.0272\n",
      "Epoch 194/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0225 - val_loss: 0.0266\n",
      "Epoch 195/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0238 - val_loss: 0.0265\n",
      "Epoch 196/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0236 - val_loss: 0.0265: 0 - ETA: 1s - loss: - ETA:\n",
      "Epoch 197/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0241 - val_loss: 0.0272\n",
      "Epoch 198/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0231 - val_loss: 0.0264\n",
      "Epoch 199/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0237 - val_loss: 0.0254\n",
      "Epoch 200/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0233 - val_loss: 0.0268\n",
      "Epoch 201/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0241 - val_loss: 0.0260\n",
      "Epoch 202/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0237 - val_loss: 0.0264.023 - \n",
      "Epoch 203/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0224 - val_loss: 0.0292A: 0s - loss: 0. - ETA: 0s - loss: 0.02\n",
      "Epoch 204/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0241 - val_loss: 0.0273\n",
      "Epoch 205/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0233 - val_loss: 0.0260\n",
      "Epoch 206/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0230 - val_loss: 0.0261\n",
      "Epoch 207/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0237 - val_loss: 0.0259: 1s - - ETA: 0s - los - ETA: 0s - loss: 0.0 - ETA: 0s - loss: 0.0\n",
      "Epoch 208/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0235 - val_loss: 0.0275\n",
      "Epoch 209/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0227 - val_loss: 0.0269\n",
      "Epoch 210/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0240 - val_loss: 0.0268 loss: 0 - ETA: 6s - loss - ETA: - ET - ETA: 0s - loss:\n",
      "Epoch 211/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0242 - val_loss: 0.0245 0s - \n",
      "Epoch 212/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0236 - val_loss: 0.0270\n",
      "Epoch 213/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0233 - val_loss: 0.0252\n",
      "Epoch 214/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0231 - val_loss: 0.0247TA: 0s - los\n",
      "Epoch 215/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0236 - val_loss: 0.0265\n",
      "Epoch 216/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0227 - val_loss: 0.0256\n",
      "Epoch 217/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0237 - val_loss: 0.0249\n",
      "Epoch 218/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0233 - val_loss: 0.0247ET\n",
      "Epoch 219/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0238 - val_loss: 0.0257\n",
      "Epoch 220/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0234 - val_loss: 0.0263\n",
      "Epoch 221/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0231 - val_loss: 0.0252\n",
      "Epoch 222/1000\n",
      "3232/3232 [==============================] - 69s 22ms/sample - loss: 0.0240 - val_loss: 0.0278\n",
      "Epoch 223/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0238 - val_loss: 0.0285\n",
      "Epoch 224/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0239 - val_loss: 0.0289\n",
      "Epoch 225/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0239 - val_loss: 0.0243\n",
      "Epoch 226/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0230 - val_loss: 0.0278\n",
      "Epoch 227/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0233 - val_loss: 0.0285- loss: - ETA: 7s -  - ETA: 5s - loss: 0.025 - ETA: 5s - - ETA: 1s -  - ETA: 0s - - ETA: 0s - loss: 0.0\n",
      "Epoch 228/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0227 - val_loss: 0.0261\n",
      "Epoch 229/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0245 - val_loss: 0.0258 - loss: 0.02 - ETA: 0s - loss: 0.02\n",
      "Epoch 230/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0230 - val_loss: 0.0243\n",
      "Epoch 231/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0234 - val_loss: 0.0260\n",
      "Epoch 232/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0237 - val_loss: 0.0269A: 1s - los - ETA: 0s - lo\n",
      "Epoch 233/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0219 - val_loss: 0.0259\n",
      "Epoch 234/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0237 - val_loss: 0.0250A: 0s - loss: 0.\n",
      "Epoch 235/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0229 - val_loss: 0.0268\n",
      "Epoch 236/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0230 - val_loss: 0.0262\n",
      "Epoch 237/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0236 - val_loss: 0.0253\n",
      "Epoch 238/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0231 - val_loss: 0.0261\n",
      "Epoch 239/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0233 - val_loss: 0.0243: 1s - loss:  - ETA: 0s - l\n",
      "Epoch 240/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0235 - val_loss: 0.0271\n",
      "Epoch 241/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0235 - val_loss: 0.0266s - loss: 0.023 -\n",
      "Epoch 242/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0234 - val_loss: 0.0250\n",
      "Epoch 243/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0237 - val_loss: 0.0255: 1s  - ETA: 0s - lo\n",
      "Epoch 244/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0232 - val_loss: 0.0309\n",
      "Epoch 245/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0230 - val_loss: 0.0249- ETA: 0s - \n",
      "Epoch 246/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0233 - val_loss: 0.0265\n",
      "Epoch 247/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0236 - val_loss: 0.0262\n",
      "Epoch 248/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0242 - val_loss: 0.0263: 0s - loss: 0.02 - ETA: 0s - loss: 0.\n",
      "Epoch 249/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0237 - val_loss: 0.0250ss - ETA: 1s - loss: 0.02 - ETA: 1s -  - ETA: 0s - loss:\n",
      "Epoch 250/1000\n",
      "3232/3232 [==============================] - 69s 22ms/sample - loss: 0.0231 - val_loss: 0.0262\n",
      "Epoch 251/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0229 - val_loss: 0.0260 - loss:\n",
      "Epoch 252/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0227 - val_loss: 0.0276\n",
      "Epoch 253/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0243 - val_loss: 0.0254\n",
      "Epoch 254/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0228 - val_loss: 0.0259\n",
      "Epoch 255/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0239 - val_loss: 0.0273 0\n",
      "Epoch 256/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0235 - val_loss: 0.0258\n",
      "Epoch 257/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0229 - val_loss: 0.0261TA: 0s - loss: 0.023 - ETA: 0s - l\n",
      "Epoch 258/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0237 - val_loss: 0.0258\n",
      "Epoch 259/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0240 - val_loss: 0.0244\n",
      "Epoch 260/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0231 - val_loss: 0.0260\n",
      "Epoch 261/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0238 - val_loss: 0.0271\n",
      "Epoch 262/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0238 - val_loss: 0.0315\n",
      "Epoch 263/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0234 - val_loss: 0.0270\n",
      "Epoch 264/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0234 - val_loss: 0.0253\n",
      "Epoch 265/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0239 - val_loss: 0.0255\n",
      "Epoch 266/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0237 - val_loss: 0.02400s - loss: 0.023\n",
      "Epoch 267/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0238 - val_loss: 0.0262\n",
      "Epoch 268/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0241 - val_loss: 0.0245\n",
      "Epoch 269/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0235 - val_loss: 0.0260\n",
      "Epoch 270/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0241 - val_loss: 0.0261\n",
      "Epoch 271/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0234 - val_loss: 0.0253\n",
      "Epoch 272/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0232 - val_loss: 0.0241\n",
      "Epoch 273/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0227 - val_loss: 0.0243\n",
      "Epoch 274/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0237 - val_loss: 0.02533s - lo - ETA: 0\n",
      "Epoch 275/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0261 - val_loss: 0.0244\n",
      "Epoch 276/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0224 - val_loss: 0.0251\n",
      "Epoch 277/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0238 - val_loss: 0.0243\n",
      "Epoch 278/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0237 - val_loss: 0.0249s: 0 - ETA: 0s - lo - ETA: 0s - loss: 0.0\n",
      "Epoch 279/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0228 - val_loss: 0.0240\n",
      "Epoch 280/1000\n",
      "3232/3232 [==============================] - 69s 22ms/sample - loss: 0.0236 - val_loss: 0.0272ss - ETA: 12s  \n",
      "Epoch 281/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0229 - val_loss: 0.0270ETA: 1s - ETA: 0s - l\n",
      "Epoch 282/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0225 - val_loss: 0.0266\n",
      "Epoch 283/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0245 - val_loss: 0.0263\n",
      "Epoch 284/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0236 - val_loss: 0.0243\n",
      "Epoch 285/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0229 - val_loss: 0.0262\n",
      "Epoch 286/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0236 - val_loss: 0.0244\n",
      "Epoch 287/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0225 - val_loss: 0.0243A: 4s - lo - ETA: 4s - loss:  - ETA:  - - ET\n",
      "Epoch 288/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0238 - val_loss: 0.0253\n",
      "Epoch 289/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0238 - val_loss: 0.0265\n",
      "Epoch 290/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0242 - val_loss: 0.0250\n",
      "Epoch 291/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0231 - val_loss: 0.0248\n",
      "Epoch 292/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0241 - val_loss: 0.0258\n",
      "Epoch 293/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0230 - val_loss: 0.0252 - lo\n",
      "Epoch 294/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0234 - val_loss: 0.0245\n",
      "Epoch 295/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0231 - val_loss: 0.0255\n",
      "Epoch 296/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0230 - val_loss: 0.0243- loss: - ETA: 1s  - ETA: 0s - loss: 0.\n",
      "Epoch 297/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0225 - val_loss: 0.0245\n",
      "Epoch 298/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0224 - val_loss: 0.0257ETA: 3s - loss:  - ET - ETA: 0s - \n",
      "Epoch 299/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0238 - val_loss: 0.0250\n",
      "Epoch 300/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0230 - val_loss: 0.0257\n",
      "Epoch 301/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0236 - val_loss: 0.0245\n",
      "Epoch 302/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0226 - val_loss: 0.0251TA: 1s - loss: 0.023 - ETA: 1s  - ETA: 0s - loss: 0.\n",
      "Epoch 303/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0246 - val_loss: 0.0256: 0s - l\n",
      "Epoch 304/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0230 - val_loss: 0.0261\n",
      "Epoch 305/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0236 - val_loss: 0.0257\n",
      "Epoch 306/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0242 - val_loss: 0.0254\n",
      "Epoch 307/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0248 - val_loss: 0.0267\n",
      "Epoch 308/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0239 - val_loss: 0.0252\n",
      "Epoch 309/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0235 - val_loss: 0.0247\n",
      "Epoch 310/1000\n",
      "3232/3232 [==============================] - 69s 22ms/sample - loss: 0.0235 - val_loss: 0.0256\n",
      "Epoch 311/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0229 - val_loss: 0.0255\n",
      "Epoch 312/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0234 - val_loss: 0.0269\n",
      "Epoch 313/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0226 - val_loss: 0.0263\n",
      "Epoch 314/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0231 - val_loss: 0.0251\n",
      "Epoch 315/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0236 - val_loss: 0.0248\n",
      "Epoch 316/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0228 - val_loss: 0.0256\n",
      "Epoch 317/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0231 - val_loss: 0.0248\n",
      "Epoch 318/1000\n",
      "3232/3232 [==============================] - 71s 22ms/sample - loss: 0.0246 - val_loss: 0.0278\n",
      "Epoch 319/1000\n",
      "3232/3232 [==============================] - 69s 21ms/sample - loss: 0.0240 - val_loss: 0.0261\n",
      "Epoch 320/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0246 - val_loss: 0.0253\n",
      "Epoch 321/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0237 - val_loss: 0.0278\n",
      "Epoch 322/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0248 - val_loss: 0.0261\n",
      "Epoch 323/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0232 - val_loss: 0.0252\n",
      "Epoch 324/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0235 - val_loss: 0.0253\n",
      "Epoch 325/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0224 - val_loss: 0.0252\n",
      "Epoch 326/1000\n",
      "3232/3232 [==============================] - 74s 23ms/sample - loss: 0.0229 - val_loss: 0.0262\n",
      "Epoch 327/1000\n",
      "3232/3232 [==============================] - 76s 24ms/sample - loss: 0.0236 - val_loss: 0.0254\n",
      "Epoch 328/1000\n",
      "3232/3232 [==============================] - 77s 24ms/sample - loss: 0.0233 - val_loss: 0.0250\n",
      "Epoch 329/1000\n",
      "3232/3232 [==============================] - 77s 24ms/sample - loss: 0.0231 - val_loss: 0.0245\n",
      "Epoch 330/1000\n",
      "3232/3232 [==============================] - 77s 24ms/sample - loss: 0.0221 - val_loss: 0.0268\n",
      "Epoch 331/1000\n",
      "3232/3232 [==============================] - 79s 24ms/sample - loss: 0.0241 - val_loss: 0.0241\n",
      "Epoch 332/1000\n",
      "3232/3232 [==============================] - 77s 24ms/sample - loss: 0.0229 - val_loss: 0.0264\n",
      "Epoch 333/1000\n",
      "3232/3232 [==============================] - 75s 23ms/sample - loss: 0.0230 - val_loss: 0.0285\n",
      "Epoch 334/1000\n",
      "3232/3232 [==============================] - 74s 23ms/sample - loss: 0.0237 - val_loss: 0.0247\n",
      "Epoch 335/1000\n",
      "3232/3232 [==============================] - 77s 24ms/sample - loss: 0.0242 - val_loss: 0.0251\n",
      "Epoch 336/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0230 - val_loss: 0.0254\n",
      "Epoch 337/1000\n",
      "3232/3232 [==============================] - 96s 30ms/sample - loss: 0.0234 - val_loss: 0.0257\n",
      "Epoch 338/1000\n",
      "3232/3232 [==============================] - 90s 28ms/sample - loss: 0.0234 - val_loss: 0.0256\n",
      "Epoch 339/1000\n",
      "3232/3232 [==============================] - 72s 22ms/sample - loss: 0.0233 - val_loss: 0.0253\n",
      "Epoch 340/1000\n",
      "3232/3232 [==============================] - 72s 22ms/sample - loss: 0.0229 - val_loss: 0.0244\n",
      "Epoch 341/1000\n",
      "3232/3232 [==============================] - 73s 22ms/sample - loss: 0.0238 - val_loss: 0.0241\n",
      "Epoch 342/1000\n",
      "3232/3232 [==============================] - 74s 23ms/sample - loss: 0.0231 - val_loss: 0.0251\n",
      "Epoch 343/1000\n",
      "3232/3232 [==============================] - 71s 22ms/sample - loss: 0.0235 - val_loss: 0.0243\n",
      "Epoch 344/1000\n",
      "3232/3232 [==============================] - 72s 22ms/sample - loss: 0.0231 - val_loss: 0.0239\n",
      "Epoch 345/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232/3232 [==============================] - 71s 22ms/sample - loss: 0.0224 - val_loss: 0.0260\n",
      "Epoch 346/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0224 - val_loss: 0.0243\n",
      "Epoch 347/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0232 - val_loss: 0.0240\n",
      "Epoch 348/1000\n",
      "3232/3232 [==============================] - 70s 22ms/sample - loss: 0.0226 - val_loss: 0.0249\n",
      "Epoch 349/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0230 - val_loss: 0.0243\n",
      "Epoch 350/1000\n",
      "3232/3232 [==============================] - 89s 27ms/sample - loss: 0.0242 - val_loss: 0.0252\n",
      "Epoch 351/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0234 - val_loss: 0.0243ETA: 3s - loss: 0.023 -  \n",
      "Epoch 352/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0227 - val_loss: 0.0282 2s  - ETA: 1s - - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.\n",
      "Epoch 353/1000\n",
      "3232/3232 [==============================] - 89s 27ms/sample - loss: 0.0229 - val_loss: 0.0247\n",
      "Epoch 354/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0236 - val_loss: 0.0247\n",
      "Epoch 355/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0233 - val_loss: 0.0254\n",
      "Epoch 356/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0231 - val_loss: 0.0246- l - ETA: 1s - loss - ETA: \n",
      "Epoch 357/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0226 - val_loss: 0.0258\n",
      "Epoch 358/1000\n",
      "3232/3232 [==============================] - 89s 27ms/sample - loss: 0.0233 - val_loss: 0.0248 - loss: - ETA: 2 -\n",
      "Epoch 359/1000\n",
      "3232/3232 [==============================] - 89s 27ms/sample - loss: 0.0232 - val_loss: 0.0253 8s - loss: 0.0 - - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.0\n",
      "Epoch 360/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0229 - val_loss: 0.0246- loss: - ETA: 0s - loss: 0.022\n",
      "Epoch 361/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0229 - val_loss: 0.0253\n",
      "Epoch 362/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0226 - val_loss: 0.0243 8s  - ETA: 7s - loss: 0.022 - ETA: 7s - ETA - E - ETA: 2s - - ETA: 1s - los - ETA: 0s - l\n",
      "Epoch 363/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0234 - val_loss: 0.0268A: 0s - loss:\n",
      "Epoch 364/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0234 - val_loss: 0.0252\n",
      "Epoch 365/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0234 - val_loss: 0.0254loss: 0.\n",
      "Epoch 366/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0241 - val_loss: 0.0243TA: 12s - loss: 0.02 - ETA:  - ETA: 7s - loss: 0. - ETA: 7s  - ETA: - ETA: 4s - loss: 0.024 - ETA: 3s - ET - ETA: 1s  - ETA: 0s - loss:  - ETA: 0s - loss:\n",
      "Epoch 367/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0224 - val_loss: 0.0252 - ETA: 10s -  - - ETA: 9s  - ETA - ETA: 7s - loss: 0  - ETA: 5s - ETA: 4s - l - ETA: 3s - l - ET - ETA: 1s - ETA: 0s -\n",
      "Epoch 368/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0230 - val_loss: 0.0250\n",
      "Epoch 369/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0232 - val_loss: 0.0258s - loss\n",
      "Epoch 370/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0230 - val_loss: 0.0261- ETA: 9s - loss: 0.02 - ETA: 9s -  - ETA: 8s - loss: - ETA: 8s - los - ETA: - E -  - ETA: 0s - loss: 0\n",
      "Epoch 371/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0237 - val_loss: 0.0258: 4s - loss: 0. - ETA: 4s - - ETA: 3 - ETA: 2s - loss: 0.023 - ETA\n",
      "Epoch 372/1000\n",
      "3232/3232 [==============================] - 89s 27ms/sample - loss: 0.0228 - val_loss: 0.0254 - loss: - ETA:  - ETA: 5s - loss:\n",
      "Epoch 373/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0229 - val_loss: 0.0258\n",
      "Epoch 374/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0227 - val_loss: 0.0270TA: 9s - loss - ETA: 6s -  - ETA: 5s - loss:  - ETA - ET - ETA: 2 - ETA: 1s - loss: - ETA: 0s - - ETA: 0s - loss: 0.02\n",
      "Epoch 375/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0240 - val_loss: 0.0259: 13 - ETA: 12  - ETA: 2s - loss - ETA: 2s - loss: 0.024 - ETA: 2s - lo\n",
      "Epoch 376/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0235 - val_loss: 0.0272A: 9s -  - ET - ET - ETA: 6s - l - ETA: 2s - loss: 0. - ETA: 1s - loss: 0.02 - ETA: 1s - - ETA: 0s - - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.023\n",
      "Epoch 377/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0227 - val_loss: 0.0261TA: 0s - - ETA: 0s - loss: 0.0\n",
      "Epoch 378/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0228 - val_loss: 0.024635s -  - ETA: 27s - loss: 0.02 - - - ETA - ETA:  - ETA - ETA: 2s - loss: 0.0 - ETA: 2s -\n",
      "Epoch 379/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0236 - val_loss: 0.0243 - ETA: 9s - loss: 0.0 - ETA: 9s  - ETA: 8s - lo - ETA: 7s - - ETA: 6s - loss - ETA: 6s - - ETA: 5s -\n",
      "Epoch 380/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0230 - val_loss: 0.0248- ETA: 9s - loss: 0.02 - ETA: 9s -  - ETA - ETA: 2s - - ETA: 2s - los - ETA: 1 - ETA: 0s - loss: 0 - ETA: 0s - loss: 0.\n",
      "Epoch 381/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0227 - val_loss: 0.0242\n",
      "Epoch 382/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0228 - val_loss: 0.0246- loss: 0.0 - ETA: 1s - loss: 0.0 - ETA: 0s - loss: 0.0\n",
      "Epoch 383/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0240 - val_loss: 0.0275 loss: 0 - ETA:  - ETA: 6s - loss: 0 - ETA: 6s - loss: - ET - ET\n",
      "Epoch 384/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0238 - val_loss: 0.0253oss: - ETA: 7s -  - E - ETA: 3s - loss: 0.02 - ETA: 3s - loss: - ETA: 1s - loss: 0. - ETA: \n",
      "Epoch 385/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0230 - val_loss: 0.0252\n",
      "Epoch 386/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0244 - val_loss: 0.0269\n",
      "Epoch 387/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0233 - val_loss: 0.0251\n",
      "Epoch 388/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0227 - val_loss: 0.0244\n",
      "Epoch 389/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0230 - val_loss: 0.02432 - ETA: 5s - loss: 0 - ETA: 4s - - ETA -  - ETA: 1s - loss: 0.0 - ETA\n",
      "Epoch 390/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0235 - val_loss: 0.0248- los - ETA: 4s - l - ETA: 3s - loss: 0. - ETA: 3s - loss - E\n",
      "Epoch 391/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0235 - val_loss: 0.0235lo - ETA: 10s - l - ETA: 9s - ETA  - ETA:  - ETA: 0s - loss:  - ETA: 0s - loss: 0.02\n",
      "Epoch 392/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0236 - val_loss: 0.0240\n",
      "Epoch 393/1000\n",
      "3232/3232 [==============================] - 86s 26ms/sample - loss: 0.0235 - val_loss: 0.0244: 1s - los - ETA: 0s - l\n",
      "Epoch 394/1000\n",
      "3232/3232 [==============================] - 86s 26ms/sample - loss: 0.0228 - val_loss: 0.024912s -  - ETA: 2s - loss: 0 - ETA: 2s - loss: 0.023 - ETA: 2s - loss: 0 - ETA: 1s - loss: 0\n",
      "Epoch 395/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0239 - val_loss: 0.0248\n",
      "Epoch 396/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0244 - val_loss: 0.0263: 10s - loss:  - ETA: 10s - loss: 0.02 - ETA: 10 - ETA: 9s - lo - ETA: 8s -  - ETA: 8s -  - ETA: 5s - loss: 0 - ETA: 3s  - ETA: 3s - l - ETA: 2s - - ETA: 1s - loss: - ETA: 1s  - ETA: 0s - loss: \n",
      "Epoch 397/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0222 - val_loss: 0.0269\n",
      "Epoch 398/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0241 - val_loss: 0.0264TA: 10s -  - ETA: 9s - loss:  - ETA: - ETA: 7 - ETA: 6s - los - ETA: 4s -  - ETA: 4s - loss: 0. - ETA: 3s - - ETA: 3s - loss: 0. - ETA: - ETA: 1s - loss: 0.0\n",
      "Epoch 399/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0228 - val_loss: 0.0252 ETA: 8 - ETA: 5s - loss: 0.023 - ETA: 5s - loss: 0.0 - ETA: 5s - l - ETA: 3s - loss: 0 - ETA: 3s - loss:  - E - ETA: 1s - loss: - ETA: 0\n",
      "Epoch 400/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0233 - val_loss: 0.0248E - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.\n",
      "Epoch 401/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0229 - val_loss: 0.0247: - ETA: 0s - loss: 0.022 - ETA: 0s - loss\n",
      "Epoch 402/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0224 - val_loss: 0.0247- loss: 0.022 - ETA: 1s - loss: 0.02 - ET\n",
      "Epoch 403/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0234 - val_loss: 0.0251 - ETA: 11s - loss:  - ETA: 11s  - ETA: 11s - loss:  - E - ETA: 8s - loss: 0. - ETA: 8s - lo - ETA: 8s -  - ETA: 7s - - ETA: 4s - los - \n",
      "Epoch 404/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0227 - val_loss: 0.0270 - ETA:  - ETA: 31s - loss:  - - ETA: 26s - loss: 0. - ETA:  - ETA: 20 - ETA: 20 - ETA:  - ETA: 9s - loss: - ETA: 9s - loss: 0. - ETA: 8s - lo - ETA: 8s - loss: 0. - ETA: 8s - loss: 0.02 - ET - ETA: 6s - loss: 0. - ETA: 6s - loss: 0 - ETA: 6s - - ETA: 5s - loss: 0. - ETA: 5s - loss: 0. - ETA: 4 - - ETA: 0s \n",
      "Epoch 405/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0234 - val_loss: 0.0252- loss: 0.024 -  - ETA: 5s - loss: 0.023 - ETA: 5s - loss: 0.0 - ETA: - ETA: 4 - E - ETA: 2s - loss: 0.02 - ETA: 1s - loss: 0. - ETA: 1s - loss: 0 - ETA: 1s - ETA: 0s - loss: 0.0 - ETA: 0s - loss: 0.\n",
      "Epoch 406/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0235 - val_loss: 0.0267ETA: 1s - lo - ETA: 1s - loss: 0.023 - ETA: 1s - loss: 0.023 - ETA: 1s - loss: 0.02 - ETA: 0s - l - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.0\n",
      "Epoch 407/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0246 - val_loss: 0.0238TA:  - ETA: 13s - loss: 0.02 - ETA: 13s - loss: 0. - ETA: 9s  -  - ETA: 4s  - ETA: 3s - loss: 0.02 - ETA: 3s  - ETA: 2s - - E - ETA: 0s - l\n",
      "Epoch 408/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0241 - val_loss: 0.0241los - ETA: 7s - loss - ETA: 6s - lo - ETA: 6s - loss: 0.0 - ETA: - ETA: 4 - ETA: 3s - los - ETA: 3s - - ETA: 0s \n",
      "Epoch 409/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0228 - val_loss: 0.0260 - loss: 0.023  - ETA: 2s - loss - ETA: 2s - l\n",
      "Epoch 410/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0233 - val_loss: 0.0251A: 8s - loss: 0.021 - ETA: 8s - loss: 0 - ETA: 8s -  - ETA: 7s  - ETA: 0s - loss\n",
      "Epoch 411/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0231 - val_loss: 0.0250\n",
      "Epoch 412/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0229 - val_loss: 0.0243ETA\n",
      "Epoch 413/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0239 - val_loss: 0.0254A: 1s - loss: - ETA: 0s - \n",
      "Epoch 414/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0238 - val_loss: 0.0313\n",
      "Epoch 415/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0238 - val_loss: 0.0245ss: 0.02 - ETA: 1s - loss: 0 - ETA: 1s - los - ETA: 0s - los - ETA: 0s - loss: 0.02\n",
      "Epoch 416/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0229 - val_loss: 0.0330s: 0. - ET - ETA - ETA: 6 - ETA: 4s - loss: 0.022 - ETA: 4s\n",
      "Epoch 417/1000\n",
      "3232/3232 [==============================] - 86s 26ms/sample - loss: 0.0247 - val_loss: 0.0241s -  - E - ETA:  - ETA: 0s - - ETA: 0s - loss: 0.0\n",
      "Epoch 418/1000\n",
      "3232/3232 [==============================] - 107s 33ms/sample - loss: 0.0222 - val_loss: 0.0243\n",
      "Epoch 419/1000\n",
      "3232/3232 [==============================] - 109s 34ms/sample - loss: 0.0233 - val_loss: 0.0264\n",
      "Epoch 420/1000\n",
      "3232/3232 [==============================] - 109s 34ms/sample - loss: 0.0226 - val_loss: 0.0249\n",
      "Epoch 421/1000\n",
      "3232/3232 [==============================] - 110s 34ms/sample - loss: 0.0235 - val_loss: 0.0255\n",
      "Epoch 422/1000\n",
      "3232/3232 [==============================] - 110s 34ms/sample - loss: 0.0224 - val_loss: 0.02390.022\n",
      "Epoch 423/1000\n",
      "3232/3232 [==============================] - 110s 34ms/sample - loss: 0.0242 - val_loss: 0.0245\n",
      "Epoch 424/1000\n",
      "3232/3232 [==============================] - 110s 34ms/sample - loss: 0.0233 - val_loss: 0.0251\n",
      "Epoch 425/1000\n",
      "3232/3232 [==============================] - 108s 34ms/sample - loss: 0.0233 - val_loss: 0.0243\n",
      "Epoch 426/1000\n",
      "3232/3232 [==============================] - 108s 34ms/sample - loss: 0.0225 - val_loss: 0.0245\n",
      "Epoch 427/1000\n",
      "3232/3232 [==============================] - 109s 34ms/sample - loss: 0.0236 - val_loss: 0.0251\n",
      "Epoch 428/1000\n",
      "3232/3232 [==============================] - 108s 33ms/sample - loss: 0.0225 - val_loss: 0.0244\n",
      "Epoch 429/1000\n",
      "3232/3232 [==============================] - 96s 30ms/sample - loss: 0.0239 - val_loss: 0.0243TA: 7s - loss: 0.0 - ETA: 7s - l - ETA: 6s  - ETA: 5s - - ET - ETA: 3s - loss: 0.0 - ETA: 3s - loss: 0.023 - - ET\n",
      "Epoch 430/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0238 - val_loss: 0.0243 - loss:  - ETA: 5s  -  - ETA: 3s - l - ETA: 2s - loss: 0 - ETA: \n",
      "Epoch 431/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0224 - val_loss: 0.0256s - ETA: 2s - loss: 0.0 - ET\n",
      "Epoch 432/1000\n",
      "3232/3232 [==============================] - 89s 27ms/sample - loss: 0.0251 - val_loss: 0.03276s - loss - ETA: 5s - loss: 0.02 - E - ETA: 4s - loss: 0 - ETA: 4s -\n",
      "Epoch 433/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0264 - val_loss: 0.0302oss: 0 - ETA: 1s - lo - ETA: 0s - loss: 0.0 - ETA: 0s - loss - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 0.0\n",
      "Epoch 434/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0243 - val_loss: 0.0274: 1s  - ETA: 0s - loss: 0 - ETA: 0s - loss: \n",
      "Epoch 435/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0244 - val_loss: 0.0246\n",
      "Epoch 436/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0238 - val_loss: 0.0248 - ETA: 5s - loss: 0.02 - ETA: 5s - loss:  - ETA: - ETA: 0s - lo\n",
      "Epoch 437/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0228 - val_loss: 0.0247oss: 0. - E - ETA: 6s - loss -  - ETA: 3s - loss: - ETA: 2s - loss: 0. - ETA: 2s - lo - ETA: 1s - l - ETA: 1s - loss: 0.022 - ETA: 1s - loss: 0 - ETA: 0s -\n",
      "Epoch 438/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0240 - val_loss: 0.0274: 9s - loss: 0.0 - ETA: 9s -  - ET - ETA:  - ETA: 4 - ETA: 3s - los - ETA: 2s - loss: 0.0 - ETA: 1s - los - ETA: 0s - loss\n",
      "Epoch 439/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0258 - val_loss: 0.0295loss: - ETA: 9s - loss: 0. - ETA: 9 - ETA: 6s - lo -\n",
      "Epoch 440/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0251 - val_loss: 0.02570.02 - ETA: 10s - loss:  - ETA: \n",
      "Epoch 441/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0236 - val_loss: 0.0262\n",
      "Epoch 442/1000\n",
      "3232/3232 [==============================] - 86s 26ms/sample - loss: 0.0224 - val_loss: 0.0262\n",
      "Epoch 443/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0230 - val_loss: 0.0244s - loss: 0. - ETA: 10s - loss: 0. - ET - ETA: 9s - loss: 0.0 -  - ETA: 8s - loss: 0.024 - ETA: 8s - - ETA: 7s\n",
      "Epoch 444/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0234 - val_loss: 0.0247- ETA: 5s  - ETA: 4s - ETA: 2s - loss: 0.02 - ETA: 2s - loss: 0.02 - ETA: 1s - loss: 0.02 - ETA: 1s - loss: 0 - ETA: 1s -  - ETA: 0s - loss: 0.02 - ETA: 0s - lo\n",
      "Epoch 445/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0224 - val_loss: 0.0278: 0.02 - ETA: 8s - loss: 0 - ETA: 8s - loss: 0.0 - ETA: 7s - los - ETA: \n",
      "Epoch 446/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0241 - val_loss: 0.0262 ETA: 9s - los - ETA: 6s - loss: 0.0 - ETA: 2s - l - \n",
      "Epoch 447/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0224 - val_loss: 0.0236ETA: 2s - loss: 0 - ETA: 0\n",
      "Epoch 448/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0227 - val_loss: 0.0241- ETA: 9s - lo - ETA: 8s  - ETA: 7s - loss: 0.02 - ET - ETA: 4s - loss: 0.02 - ETA: 4s - loss: 0. - ETA: 4s - loss - ETA: 3s - - ETA: 3s - los - ETA: 2s - loss: - ETA: 2s - loss: 0.0 - ETA: 1s - los - E\n",
      "Epoch 449/1000\n",
      "3232/3232 [==============================] - 86s 26ms/sample - loss: 0.0233 - val_loss: 0.0258loss - ETA - ETA: 1 - ETA: 9s - loss: 0. - ETA: 9s - l - ETA: 8s - loss: 0. - ETA: 8s - - ETA: 6s - - ETA: 5s - loss: 0 - ETA: 5s - loss: 0. - ETA: 4s - loss: -\n",
      "Epoch 450/1000\n",
      "3232/3232 [==============================] - 86s 26ms/sample - loss: 0.0241 - val_loss: 0.0247\n",
      "Epoch 451/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0226 - val_loss: 0.0265ss: 0.0 - ETA: 8s - lo - ETA: 2s - los - ETA: 2s - \n",
      "Epoch 452/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0244 - val_loss: 0.0249: 0.024 - ETA: 8s - l - ETA: 7s - loss: 0 - ET - ETA: 4s - loss: 0. - ETA: 4s - lo - ETA: 0s - los - ETA: 0s - loss: 0.0\n",
      "Epoch 453/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0238 - val_loss: 0.0263\n",
      "Epoch 454/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0230 - val_loss: 0.0247\n",
      "Epoch 455/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0224 - val_loss: 0.0265A: 1s - ETA: 0s - loss:  - ETA: 0s - loss:\n",
      "Epoch 456/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0236 - val_loss: 0.0244\n",
      "Epoch 457/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0230 - val_loss: 0.0246: 3s - loss: 0 - ETA: 2s - loss: 0.0 - ETA:\n",
      "Epoch 458/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0226 - val_loss: 0.0247\n",
      "Epoch 459/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0228 - val_loss: 0.0253TA: 10s - loss: 0.0 - ETA: 9s - loss: 0.02 - ETA: 9s - ETA: 8s - l - ETA: 8s - loss: 0.02 - ETA: 4s -  - ETA: 4s - loss: 0.023 - ETA: 4s - l - ETA: 1s -  - ETA: 1s -  - ETA: 0s - loss: 0.0 - ETA: 0s - loss: 0.0\n",
      "Epoch 460/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0241 - val_loss: 0.0241s  - ETA: 4s - loss: 0.023 - ETA: 4s - loss:  - ETA: - ETA: 3s - los - ETA: 2s - loss: - ETA: 2s  - ETA: 1s - los - ETA: 0s - loss: 0.023 - ETA: 0s - l - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.024\n",
      "Epoch 461/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0235 - val_loss: 0.0243: 0. - ETA:  - ETA: 7s - los - ETA: 6s -  - ETA: \n",
      "Epoch 462/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0224 - val_loss: 0.0276\n",
      "Epoch 463/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0222 - val_loss: 0.0276- loss - ETA: 28s - lo - ETA: 28 - ETA: 26s\n",
      "Epoch 464/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0236 - val_loss: 0.0252\n",
      "Epoch 465/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0232 - val_loss: 0.0238\n",
      "Epoch 466/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0229 - val_loss: 0.0257A: 4 - ETA: 3s - l - ETA: 2s - loss: 0. - ETA: 2s - loss: 0.0 - ETA: 2s \n",
      "Epoch 467/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0236 - val_loss: 0.02410.022 - ETA: 7s - lo - ETA: 6s - ETA - ETA:  - ETA: 3s - lo  - ETA: 1s - - ETA: 0s - loss:\n",
      "Epoch 468/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0224 - val_loss: 0.0245- ETA: 5s - - ETA: 2s - loss: 0.02 - ET\n",
      "Epoch 469/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0231 - val_loss: 0.0247: 2s - los - ETA: 0s - loss: 0.02\n",
      "Epoch 470/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0229 - val_loss: 0.0246 ETA - ETA: 3s -  - ETA: 0s - \n",
      "Epoch 471/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0221 - val_loss: 0.0254\n",
      "Epoch 472/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0226 - val_loss: 0.0252 - ETA -\n",
      "Epoch 473/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0225 - val_loss: 0.0239\n",
      "Epoch 474/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0221 - val_loss: 0.0238TA: 0s - loss: 0.0\n",
      "Epoch 475/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0242 - val_loss: 0.0248\n",
      "Epoch 476/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0243 - val_loss: 0.0254\n",
      "Epoch 477/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0248 - val_loss: 0.0265s  - ETA: 8s - l - ETA: 5s - loss: 0.0 - ETA - ETA: 2s - loss: 0. - ETA: 2s - los - ETA: 2s - loss: 0.024 - ET - ETA: 0s -\n",
      "Epoch 478/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0238 - val_loss: 0.0245ETA:  - ETA: 3s - loss\n",
      "Epoch 479/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0231 - val_loss: 0.0254\n",
      "Epoch 480/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0228 - val_loss: 0.0287\n",
      "Epoch 481/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0234 - val_loss: 0.02420.\n",
      "Epoch 482/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0231 - val_loss: 0.0264s -  - ETA: 3s - loss: 0. - ETA: 2s - loss: 0.02 - ETA: 2s - loss: 0 - ETA: 2s \n",
      "Epoch 483/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0224 - val_loss: 0.0259\n",
      "Epoch 484/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0236 - val_loss: 0.0249\n",
      "Epoch 485/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0225 - val_loss: 0.0267\n",
      "Epoch 486/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0230 - val_loss: 0.0269\n",
      "Epoch 487/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0244 - val_loss: 0.0266\n",
      "Epoch 488/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0237 - val_loss: 0.0271- ETA: - ETA: 3s - lo - ETA:  - ETA: 2s -  - ETA: 1s - ETA: 0s - loss: 0. - ETA: 0s - loss: 0.0\n",
      "Epoch 489/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0226 - val_loss: 0.0248ETA: - ETA: 4s - - ETA: 3s - los - ETA: 0s - loss: 0.02\n",
      "Epoch 490/1000\n",
      "3232/3232 [==============================] - 86s 26ms/sample - loss: 0.0238 - val_loss: 0.0270- ETA: 6s -  - ETA: 5s - loss: - ETA: 5s - loss: 0.025 - ETA: 5s - loss:  - ETA:  - ETA: 3s - l - ETA: 3s - loss: 0.02 - ETA: 3s - lo - ETA: 0\n",
      "Epoch 491/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0238 - val_loss: 0.0261: 0.02\n",
      "Epoch 492/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0230 - val_loss: 0.0254TA: 11s  - ETA: 10s  - ETA: - ETA: 9s - ETA: 2\n",
      "Epoch 493/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0229 - val_loss: 0.0236- ETA: 6s - los - ETA: 6s -  - ETA: 0s - \n",
      "Epoch 494/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0229 - val_loss: 0.0250s:  - ET - ETA: 0s - loss: \n",
      "Epoch 495/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0229 - val_loss: 0.0281 ETA: 9 - ETA: 8s - los - ETA: 8s - los\n",
      "Epoch 496/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0241 - val_loss: 0.02508s - loss - ETA: 7s - lo - ETA: 6s - loss: 0.02 - - ETA: 2s - loss: 0.023 - ETA: 2s - loss: 0.0 - ETA: 2s - loss: 0.0 - ETA: 0s - loss: 0\n",
      "Epoch 497/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0239 - val_loss: 0.0249TA - ETA: 9 - ETA: 0s -\n",
      "Epoch 498/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0235 - val_loss: 0.0275 ETA: 2s - loss:  - ETA: 2s - lo - ETA: 1s  - ETA: 0s - loss: 0.02 - ETA: 0s - loss\n",
      "Epoch 499/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0250 - val_loss: 0.0259ETA:  - ETA: 2s - loss: 0.025 - ETA: 2s - loss: 0.025 - ETA: 2s - loss: 0.0 - ETA: 2 - ETA: 1s -  - ETA: 0s - lo\n",
      "Epoch 500/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0233 - val_loss: 0.0242:  - ETA - ETA: 0s - loss: 0.023\n",
      "Epoch 501/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0232 - val_loss: 0.0255 loss: 0.0\n",
      "Epoch 502/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0237 - val_loss: 0.0248 - loss: 0.0 - ETA:\n",
      "Epoch 503/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0237 - val_loss: 0.0239\n",
      "Epoch 504/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0229 - val_loss: 0.0245  - ETA: 3s -  - ET\n",
      "Epoch 505/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0224 - val_loss: 0.0260\n",
      "Epoch 506/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0240 - val_loss: 0.0261\n",
      "Epoch 507/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0236 - val_loss: 0.0258\n",
      "Epoch 508/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0231 - val_loss: 0.0258\n",
      "Epoch 509/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0232 - val_loss: 0.0249\n",
      "Epoch 510/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0236 - val_loss: 0.0244s  - E - ETA: 3s - loss:  - ETA: 3s - loss: 0.0 - ETA: 1s - loss: 0.02 - ETA: 1s -  - ETA: 0s - loss:\n",
      "Epoch 511/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0236 - val_loss: 0.0242\n",
      "Epoch 512/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0243 - val_loss: 0.0250\n",
      "Epoch 513/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0240 - val_loss: 0.0254 ETA: 4s - loss: 0 - ETA: 2\n",
      "Epoch 514/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0240 - val_loss: 0.0256\n",
      "Epoch 515/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0243 - val_loss: 0.0254- loss: 0 -  - ETA: 5s - - ETA: 4s - loss: 0. - ETA: 3 - ETA: 2s -  - ETA: 0s - lo\n",
      "Epoch 516/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0233 - val_loss: 0.0241 7s - loss: 0.02 - E - ETA: 3s -  - ETA: 2s - ETA: 1s - loss: 0.02 - ET\n",
      "Epoch 517/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0227 - val_loss: 0.0248TA: 8s - loss: 0. - ETA: 8s -  - ETA: - ET - ETA: 0s - los - ETA: 0s - loss: 0.\n",
      "Epoch 518/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0227 - val_loss: 0.0239 - ETA: - ET - ETA: 1s - loss - ETA: 0s \n",
      "Epoch 519/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0230 - val_loss: 0.0238loss:  - ETA: 24s  - E - ETA: 22s -  - ETA: 22s -  - ETA: 22s - loss:  - ETA: 22s - loss - ETA: 17s - lo - ETA:  - - ETA: 15 - ETA - ETA: 9s - - ETA: 4s - ETA - ETA: 0s - loss: 0.02 - ETA: 0s - los\n",
      "Epoch 520/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0233 - val_loss: 0.02569s - loss: 0.022 - ETA: 9s - l - ETA: 6s - loss: 0 - ETA: 6s - - ETA: 5s - loss: 0.02 - ETA: 5s - los - ETA: 0s - loss: \n",
      "Epoch 521/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0228 - val_loss: 0.0256ss:  - ETA: 0s - loss: 0.\n",
      "Epoch 522/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0231 - val_loss: 0.0253s - loss: 0. - - ETA: 4s - ETA: 3s - loss: 0.02 - ETA: 3s - loss: 0 - ETA: 3s - l - ETA: 2s - loss: - ETA: 2s - loss: 0 - ETA: 1s - loss: 0.\n",
      "Epoch 523/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0237 - val_loss: 0.0245 loss: 0.022 - ETA: 7s - los - ETA:  - ETA: - ETA: 1s - ETA: 0s - loss: 0.02\n",
      "Epoch 524/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0233 - val_loss: 0.0283 - ETA: 4s - loss: 0 - ETA: 2s - loss:  - ETA: 0s - loss: 0.\n",
      "Epoch 525/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0243 - val_loss: 0.0242\n",
      "Epoch 526/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0230 - val_loss: 0.0249TA: 44s - loss - ETA: 43s - lo - ETA: 41s - loss: 0. - ETA: 12s - loss - ETA: 11s  - ETA:  - ETA: 6s - loss: \n",
      "Epoch 527/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0237 - val_loss: 0.024414s - lo - ETA: 13s - loss: 0.02 - ETA: 13s - loss - - ETA: 11s -  - ETA: 9 - ETA: 8 - ETA: 7s - los - ETA: 7s -  - ETA: 4s - ETA: 3s - loss:  - ETA: 3s - loss: 0.0 - ETA: 3s - lo - ETA - ETA: 1s - loss: 0.0 - \n",
      "Epoch 528/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0236 - val_loss: 0.0247 - ETA - ETA: 10s - l - ETA: 9s - l - E - ETA: 7s - lo - ETA: 4 - ETA: 3s - loss: 0 - ETA\n",
      "Epoch 529/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0237 - val_loss: 0.0242ss: 0.02 - ETA: 39s - loss - ETA: 39s - lo - ETA: 38s - loss:  - ETA: 38s - lo - ETA:  - ETA: 36s  - ETA: 21s - loss - ETA:  - ETA:  - ET - ETA: 9s - - ETA: 9s - - ETA: 8s - loss:  - ETA:  - ETA: 0s - los\n",
      "Epoch 530/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0228 - val_loss: 0.0238: 23s - lo - E - ETA: 21s - lo - ETA: 14s - loss: 0. - ETA: 14s - loss - E - ETA: 13 - ETA:  - ETA: 10s - l - ETA: 1s - loss: - ETA: 0s - loss: 0.022 - ETA: 0s - lo\n",
      "Epoch 531/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0230 - val_loss: 0.0249- ETA: 9s - loss: - ETA: 7s - - ETA:   - ETA: 0s - loss: 0. - ETA: 0s - loss: 0.02 - ETA: 0s - loss\n",
      "Epoch 532/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0226 - val_loss: 0.0241A: 3s - loss: 0 - ETA: 3s - loss: 0.022 - ETA: 3s - loss: 0.02 - ETA: 3s - loss: -\n",
      "Epoch 533/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0229 - val_loss: 0.0237oss - ETA: 2s - ETA: 1s - ETA: 0s\n",
      "Epoch 534/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0228 - val_loss: 0.02538s - loss: - ETA: 8s -  - ETA: 7s - loss: 0.02 - ET - ETA: 4s - loss: 0.02 - ETA: 4s - ETA: 3s - l - ETA: 3s - loss: 0.022 - ETA: 3s - loss: - ETA: 1s - l - ETA: 0s - loss: \n",
      "Epoch 535/1000\n",
      "3232/3232 [==============================] - 86s 26ms/sample - loss: 0.0244 - val_loss: 0.0256 -\n",
      "Epoch 536/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0239 - val_loss: 0.0253- loss: 0 - ETA: 5s - loss: 0.023  - ETA: 3s - loss: 0.023 - ETA: 3s - lo - E - ETA - ETA: 0s - loss\n",
      "Epoch 537/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0235 - val_loss: 0.0251: 9s  - ETA: 7s - loss: 0.023 - ETA: 7s - los - ETA: 6s - - ETA: 4s - los - ETA: 3s - loss \n",
      "Epoch 538/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0230 - val_loss: 0.0252  - - ETA: 0s -\n",
      "Epoch 539/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0230 - val_loss: 0.0263 4s -  - ETA: 3s - loss - ETA: 2s - los - ETA: 2s - - ETA: 1s - loss: 0. - ETA: 1s  - ETA: 0s - loss:\n",
      "Epoch 540/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0235 - val_loss: 0.0258oss: 0.0 - ETA: 6s - loss:  - - ETA: 4s - loss: 0. - ETA: 4s - loss: 0.023 - ETA: 3s - loss: 0.02 - ETA: 3s - los - ETA: 3s - los - ETA: - ETA: 1s - loss: 0.02 - ETA: 1s - ETA: 0s - loss\n",
      "Epoch 541/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0239 - val_loss: 0.0239 0s - loss: 0.023\n",
      "Epoch 542/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0228 - val_loss: 0.0245 1:05 - loss: 0 - ETA - ETA: 1:03 - loss: 0.017 - ETA: 1:03 - loss: - E - E - E - ETA: 57s - loss: 0. - ETA: 57s - loss: 0.02 - ETA: 57s -  - ETA: 54s - loss:  - ETA:  - ETA:  - ETA:  - ETA: 26s - loss:  - - ETA:  - ETA: 15s - loss - ETA: 14s - lo - ETA: 13 - ETA: 12s  - ETA: 12s -  - - ETA: 1 - ETA: 9s - loss: 0.02 - ETA: 9s - loss:  - ETA: 7s - loss: 0.022 - ETA: 7s\n",
      "Epoch 543/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0241 - val_loss: 0.0262s - loss - ETA: 0s - loss: 0 - ETA: 0s - loss:\n",
      "Epoch 544/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0227 - val_loss: 0.0259TA: 8s - loss: 0.02 - ETA: 8s - loss: 0.02 - ETA: 8s - - ETA: 4s - loss: - ETA: 4s - - ETA: 3s -  - ETA: 2\n",
      "Epoch 545/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0231 - val_loss: 0.0261TA: 5s - loss - ETA: 5 - ETA: 4 - ETA: 3s - loss: 0.0 - ETA: 2s - loss: 0.023 - ETA: 2s - loss - ETA: 2s \n",
      "Epoch 546/1000\n",
      "3232/3232 [==============================] - 86s 26ms/sample - loss: 0.0229 - val_loss: 0.0260\n",
      "Epoch 547/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0231 - val_loss: 0.0270loss: 0. - ETA: 17 - ETA: 11s - lo - - ETA:\n",
      "Epoch 548/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0235 - val_loss: 0.0269 ETA: 0s - loss:\n",
      "Epoch 549/1000\n",
      "3232/3232 [==============================] - 86s 26ms/sample - loss: 0.0225 - val_loss: 0.0244 - ETA:  - ET - ETA - ETA: 2s  \n",
      "Epoch 550/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0249 - val_loss: 0.0246s - loss: 0\n",
      "Epoch 551/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0236 - val_loss: 0.0273\n",
      "Epoch 552/1000\n",
      "3232/3232 [==============================] - 90s 28ms/sample - loss: 0.0241 - val_loss: 0.0272\n",
      "Epoch 553/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0232 - val_loss: 0.0273\n",
      "Epoch 554/1000\n",
      "3232/3232 [==============================] - 93s 29ms/sample - loss: 0.0240 - val_loss: 0.0251\n",
      "Epoch 555/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0264 - val_loss: 0.0243l - ETA: 1s - loss: 0.\n",
      "Epoch 556/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0236 - val_loss: 0.0249E\n",
      "Epoch 557/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0230 - val_loss: 0.0248\n",
      "Epoch 558/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0230 - val_loss: 0.0240\n",
      "Epoch 559/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0238 - val_loss: 0.0249\n",
      "Epoch 560/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0235 - val_loss: 0.0253\n",
      "Epoch 561/1000\n",
      "3232/3232 [==============================] - 89s 27ms/sample - loss: 0.0233 - val_loss: 0.0269\n",
      "Epoch 562/1000\n",
      "3232/3232 [==============================] - 90s 28ms/sample - loss: 0.0224 - val_loss: 0.0238\n",
      "Epoch 563/1000\n",
      "3232/3232 [==============================] - 89s 27ms/sample - loss: 0.0236 - val_loss: 0.0257 - loss:\n",
      "Epoch 564/1000\n",
      "3232/3232 [==============================] - 93s 29ms/sample - loss: 0.0241 - val_loss: 0.0244\n",
      "Epoch 565/1000\n",
      "3232/3232 [==============================] - 91s 28ms/sample - loss: 0.0234 - val_loss: 0.0269- l - ETA: 0s - loss: 0.0\n",
      "Epoch 566/1000\n",
      "3232/3232 [==============================] - 91s 28ms/sample - loss: 0.0227 - val_loss: 0.0243\n",
      "Epoch 567/1000\n",
      "3232/3232 [==============================] - 99s 31ms/sample - loss: 0.0225 - val_loss: 0.0261\n",
      "Epoch 568/1000\n",
      "3232/3232 [==============================] - 124s 39ms/sample - loss: 0.0235 - val_loss: 0.0252\n",
      "Epoch 569/1000\n",
      "3232/3232 [==============================] - 130s 40ms/sample - loss: 0.0237 - val_loss: 0.0259\n",
      "Epoch 570/1000\n",
      "3232/3232 [==============================] - 102s 31ms/sample - loss: 0.0234 - val_loss: 0.0279\n",
      "Epoch 571/1000\n",
      "3232/3232 [==============================] - 100s 31ms/sample - loss: 0.0232 - val_loss: 0.0269\n",
      "Epoch 572/1000\n",
      "3232/3232 [==============================] - 98s 30ms/sample - loss: 0.0248 - val_loss: 0.0257\n",
      "Epoch 573/1000\n",
      "3232/3232 [==============================] - 94s 29ms/sample - loss: 0.0235 - val_loss: 0.0249\n",
      "Epoch 574/1000\n",
      "3232/3232 [==============================] - 94s 29ms/sample - loss: 0.0231 - val_loss: 0.0270\n",
      "Epoch 575/1000\n",
      "3232/3232 [==============================] - 94s 29ms/sample - loss: 0.0235 - val_loss: 0.0261\n",
      "Epoch 576/1000\n",
      "3232/3232 [==============================] - 94s 29ms/sample - loss: 0.0224 - val_loss: 0.0255\n",
      "Epoch 577/1000\n",
      "3232/3232 [==============================] - 93s 29ms/sample - loss: 0.0230 - val_loss: 0.0255\n",
      "Epoch 578/1000\n",
      "3232/3232 [==============================] - 94s 29ms/sample - loss: 0.0223 - val_loss: 0.0241\n",
      "Epoch 579/1000\n",
      "3232/3232 [==============================] - 93s 29ms/sample - loss: 0.0237 - val_loss: 0.0267\n",
      "Epoch 580/1000\n",
      "3232/3232 [==============================] - 93s 29ms/sample - loss: 0.0222 - val_loss: 0.0242\n",
      "Epoch 581/1000\n",
      "3232/3232 [==============================] - 94s 29ms/sample - loss: 0.0241 - val_loss: 0.0256\n",
      "Epoch 582/1000\n",
      "3232/3232 [==============================] - 93s 29ms/sample - loss: 0.0228 - val_loss: 0.0268\n",
      "Epoch 583/1000\n",
      "3232/3232 [==============================] - 95s 29ms/sample - loss: 0.0238 - val_loss: 0.0304\n",
      "Epoch 584/1000\n",
      "3232/3232 [==============================] - 104s 32ms/sample - loss: 0.0242 - val_loss: 0.0262\n",
      "Epoch 585/1000\n",
      "3232/3232 [==============================] - 102s 31ms/sample - loss: 0.0228 - val_loss: 0.0246\n",
      "Epoch 586/1000\n",
      "3232/3232 [==============================] - 122s 38ms/sample - loss: 0.0224 - val_loss: 0.0242\n",
      "Epoch 587/1000\n",
      "3232/3232 [==============================] - 129s 40ms/sample - loss: 0.0222 - val_loss: 0.0261\n",
      "Epoch 588/1000\n",
      "3232/3232 [==============================] - 123s 38ms/sample - loss: 0.0230 - val_loss: 0.0249\n",
      "Epoch 589/1000\n",
      "3232/3232 [==============================] - 129s 40ms/sample - loss: 0.0228 - val_loss: 0.0272\n",
      "Epoch 590/1000\n",
      "3232/3232 [==============================] - 126s 39ms/sample - loss: 0.0238 - val_loss: 0.0287\n",
      "Epoch 591/1000\n",
      "3232/3232 [==============================] - 126s 39ms/sample - loss: 0.0220 - val_loss: 0.0254\n",
      "Epoch 592/1000\n",
      "3232/3232 [==============================] - 130s 40ms/sample - loss: 0.0236 - val_loss: 0.0268\n",
      "Epoch 593/1000\n",
      "3232/3232 [==============================] - 121s 37ms/sample - loss: 0.0228 - val_loss: 0.0249\n",
      "Epoch 594/1000\n",
      "3232/3232 [==============================] - 116s 36ms/sample - loss: 0.0242 - val_loss: 0.0287\n",
      "Epoch 595/1000\n",
      "3232/3232 [==============================] - 127s 39ms/sample - loss: 0.0239 - val_loss: 0.0275\n",
      "Epoch 596/1000\n",
      "3232/3232 [==============================] - 123s 38ms/sample - loss: 0.0234 - val_loss: 0.0233\n",
      "Epoch 597/1000\n",
      "3232/3232 [==============================] - 132s 41ms/sample - loss: 0.0231 - val_loss: 0.0234\n",
      "Epoch 598/1000\n",
      "3232/3232 [==============================] - 124s 38ms/sample - loss: 0.0243 - val_loss: 0.0268\n",
      "Epoch 599/1000\n",
      "3232/3232 [==============================] - ETA: 0s - loss: 0.024 - 235s 73ms/sample - loss: 0.0242 - val_loss: 0.0244\n",
      "Epoch 600/1000\n",
      "3232/3232 [==============================] - 93s 29ms/sample - loss: 0.0234 - val_loss: 0.0243\n",
      "Epoch 601/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0220 - val_loss: 0.0240\n",
      "Epoch 602/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0224 - val_loss: 0.0239\n",
      "Epoch 603/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0233 - val_loss: 0.0249\n",
      "Epoch 604/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0224 - val_loss: 0.0258\n",
      "Epoch 605/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0243 - val_loss: 0.0272\n",
      "Epoch 606/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0249 - val_loss: 0.0264\n",
      "Epoch 607/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0228 - val_loss: 0.0261\n",
      "Epoch 608/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0240 - val_loss: 0.0258\n",
      "Epoch 609/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0223 - val_loss: 0.0257\n",
      "Epoch 610/1000\n",
      "3232/3232 [==============================] - 86s 26ms/sample - loss: 0.0239 - val_loss: 0.0249\n",
      "Epoch 611/1000\n",
      "3232/3232 [==============================] - 86s 26ms/sample - loss: 0.0235 - val_loss: 0.0252\n",
      "Epoch 612/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0232 - val_loss: 0.0250\n",
      "Epoch 613/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0243 - val_loss: 0.0256\n",
      "Epoch 614/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0233 - val_loss: 0.0260\n",
      "Epoch 615/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0228 - val_loss: 0.0244\n",
      "Epoch 616/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0233 - val_loss: 0.0262\n",
      "Epoch 617/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0238 - val_loss: 0.0265\n",
      "Epoch 618/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0228 - val_loss: 0.0281\n",
      "Epoch 619/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0237 - val_loss: 0.0249\n",
      "Epoch 620/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0235 - val_loss: 0.0259\n",
      "Epoch 621/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0227 - val_loss: 0.0283\n",
      "Epoch 622/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0241 - val_loss: 0.0274\n",
      "Epoch 623/1000\n",
      "3232/3232 [==============================] - 89s 27ms/sample - loss: 0.0229 - val_loss: 0.0243\n",
      "Epoch 624/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0225 - val_loss: 0.0261\n",
      "Epoch 625/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0236 - val_loss: 0.0266\n",
      "Epoch 626/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0233 - val_loss: 0.0256\n",
      "Epoch 627/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0236 - val_loss: 0.0265\n",
      "Epoch 628/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0233 - val_loss: 0.0250\n",
      "Epoch 629/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0222 - val_loss: 0.0259\n",
      "Epoch 630/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0227 - val_loss: 0.0235\n",
      "Epoch 631/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0234 - val_loss: 0.0248\n",
      "Epoch 632/1000\n",
      "3232/3232 [==============================] - 90s 28ms/sample - loss: 0.0227 - val_loss: 0.0249\n",
      "Epoch 633/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0241 - val_loss: 0.0249\n",
      "Epoch 634/1000\n",
      "3232/3232 [==============================] - 93s 29ms/sample - loss: 0.0223 - val_loss: 0.0242\n",
      "Epoch 635/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0230 - val_loss: 0.0254\n",
      "Epoch 636/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0236 - val_loss: 0.0246\n",
      "Epoch 637/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0232 - val_loss: 0.0237\n",
      "Epoch 638/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0247 - val_loss: 0.0254\n",
      "Epoch 639/1000\n",
      "3232/3232 [==============================] - 96s 30ms/sample - loss: 0.0236 - val_loss: 0.0259\n",
      "Epoch 640/1000\n",
      "3232/3232 [==============================] - 99s 31ms/sample - loss: 0.0224 - val_loss: 0.0245\n",
      "Epoch 641/1000\n",
      "3232/3232 [==============================] - 94s 29ms/sample - loss: 0.0233 - val_loss: 0.0236\n",
      "Epoch 642/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0244 - val_loss: 0.0274\n",
      "Epoch 643/1000\n",
      "3232/3232 [==============================] - 90s 28ms/sample - loss: 0.0247 - val_loss: 0.0241\n",
      "Epoch 644/1000\n",
      "3232/3232 [==============================] - 89s 27ms/sample - loss: 0.0227 - val_loss: 0.0247\n",
      "Epoch 645/1000\n",
      "3232/3232 [==============================] - 94s 29ms/sample - loss: 0.0224 - val_loss: 0.0262\n",
      "Epoch 646/1000\n",
      "3232/3232 [==============================] - 93s 29ms/sample - loss: 0.0236 - val_loss: 0.0247\n",
      "Epoch 647/1000\n",
      "3232/3232 [==============================] - 93s 29ms/sample - loss: 0.0230 - val_loss: 0.0256\n",
      "Epoch 648/1000\n",
      "3232/3232 [==============================] - 91s 28ms/sample - loss: 0.0223 - val_loss: 0.0236\n",
      "Epoch 649/1000\n",
      "3232/3232 [==============================] - 92s 29ms/sample - loss: 0.0224 - val_loss: 0.0246\n",
      "Epoch 650/1000\n",
      "3232/3232 [==============================] - 94s 29ms/sample - loss: 0.0226 - val_loss: 0.0242\n",
      "Epoch 651/1000\n",
      "3232/3232 [==============================] - 91s 28ms/sample - loss: 0.0248 - val_loss: 0.0297\n",
      "Epoch 652/1000\n",
      "3232/3232 [==============================] - 91s 28ms/sample - loss: 0.0237 - val_loss: 0.0264\n",
      "Epoch 653/1000\n",
      "3232/3232 [==============================] - 92s 28ms/sample - loss: 0.0241 - val_loss: 0.0262\n",
      "Epoch 654/1000\n",
      "3232/3232 [==============================] - 91s 28ms/sample - loss: 0.0242 - val_loss: 0.0252\n",
      "Epoch 655/1000\n",
      "3232/3232 [==============================] - 93s 29ms/sample - loss: 0.0230 - val_loss: 0.0238\n",
      "Epoch 656/1000\n",
      "3232/3232 [==============================] - 99s 31ms/sample - loss: 0.0235 - val_loss: 0.0254\n",
      "Epoch 657/1000\n",
      "3232/3232 [==============================] - 99s 31ms/sample - loss: 0.0231 - val_loss: 0.0254\n",
      "Epoch 658/1000\n",
      "3232/3232 [==============================] - 101s 31ms/sample - loss: 0.0236 - val_loss: 0.0261\n",
      "Epoch 659/1000\n",
      "3232/3232 [==============================] - 111s 34ms/sample - loss: 0.0243 - val_loss: 0.0260\n",
      "Epoch 660/1000\n",
      "3232/3232 [==============================] - 90s 28ms/sample - loss: 0.0236 - val_loss: 0.0243\n",
      "Epoch 661/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0248 - val_loss: 0.0259\n",
      "Epoch 662/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0238 - val_loss: 0.0263\n",
      "Epoch 663/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0246 - val_loss: 0.0242\n",
      "Epoch 664/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0238 - val_loss: 0.0242\n",
      "Epoch 665/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0237 - val_loss: 0.0252\n",
      "Epoch 666/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0234 - val_loss: 0.0249\n",
      "Epoch 667/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0239 - val_loss: 0.0278\n",
      "Epoch 668/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0247 - val_loss: 0.0237\n",
      "Epoch 669/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0254 - val_loss: 0.0264\n",
      "Epoch 670/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0245 - val_loss: 0.0240\n",
      "Epoch 671/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0235 - val_loss: 0.0247\n",
      "Epoch 672/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0236 - val_loss: 0.0256\n",
      "Epoch 673/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0232 - val_loss: 0.0240\n",
      "Epoch 674/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0237 - val_loss: 0.0252\n",
      "Epoch 675/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0244 - val_loss: 0.0270\n",
      "Epoch 676/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0232 - val_loss: 0.0284\n",
      "Epoch 677/1000\n",
      "3232/3232 [==============================] - 90s 28ms/sample - loss: 0.0224 - val_loss: 0.0240\n",
      "Epoch 678/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0231 - val_loss: 0.0255\n",
      "Epoch 679/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0231 - val_loss: 0.0243\n",
      "Epoch 680/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0240 - val_loss: 0.0249\n",
      "Epoch 681/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0220 - val_loss: 0.0252\n",
      "Epoch 682/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0238 - val_loss: 0.0283\n",
      "Epoch 683/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0241 - val_loss: 0.0250\n",
      "Epoch 684/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0248 - val_loss: 0.0280\n",
      "Epoch 685/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0244 - val_loss: 0.0248\n",
      "Epoch 686/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0247 - val_loss: 0.0252\n",
      "Epoch 687/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0228 - val_loss: 0.0260\n",
      "Epoch 688/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0243 - val_loss: 0.0245\n",
      "Epoch 689/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0234 - val_loss: 0.0251\n",
      "Epoch 690/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0230 - val_loss: 0.0247\n",
      "Epoch 691/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0239 - val_loss: 0.0267\n",
      "Epoch 692/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0234 - val_loss: 0.0248\n",
      "Epoch 693/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0227 - val_loss: 0.0280\n",
      "Epoch 694/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0235 - val_loss: 0.0272\n",
      "Epoch 695/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0230 - val_loss: 0.0256\n",
      "Epoch 696/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0232 - val_loss: 0.0251\n",
      "Epoch 697/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0232 - val_loss: 0.0282\n",
      "Epoch 698/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0238 - val_loss: 0.0246\n",
      "Epoch 699/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0246 - val_loss: 0.0270\n",
      "Epoch 700/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0238 - val_loss: 0.0287\n",
      "Epoch 701/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0249 - val_loss: 0.0268\n",
      "Epoch 702/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0238 - val_loss: 0.0253\n",
      "Epoch 703/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0236 - val_loss: 0.0236\n",
      "Epoch 704/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0230 - val_loss: 0.0241\n",
      "Epoch 705/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0234 - val_loss: 0.0242\n",
      "Epoch 706/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0224 - val_loss: 0.0269\n",
      "Epoch 707/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0233 - val_loss: 0.0255\n",
      "Epoch 708/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0234 - val_loss: 0.0265\n",
      "Epoch 709/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0229 - val_loss: 0.0243\n",
      "Epoch 710/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0229 - val_loss: 0.0270\n",
      "Epoch 711/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0235 - val_loss: 0.0246\n",
      "Epoch 712/1000\n",
      "3232/3232 [==============================] - 86s 26ms/sample - loss: 0.0245 - val_loss: 0.0256\n",
      "Epoch 713/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0239 - val_loss: 0.0242\n",
      "Epoch 714/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0224 - val_loss: 0.0250\n",
      "Epoch 715/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0239 - val_loss: 0.0243\n",
      "Epoch 716/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0240 - val_loss: 0.0256\n",
      "Epoch 717/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0245 - val_loss: 0.0245\n",
      "Epoch 718/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0225 - val_loss: 0.0261\n",
      "Epoch 719/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0242 - val_loss: 0.0288\n",
      "Epoch 720/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0242 - val_loss: 0.0244\n",
      "Epoch 721/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0230 - val_loss: 0.0258\n",
      "Epoch 722/1000\n",
      "3232/3232 [==============================] - 86s 26ms/sample - loss: 0.0235 - val_loss: 0.0261\n",
      "Epoch 723/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0231 - val_loss: 0.0265\n",
      "Epoch 724/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0232 - val_loss: 0.0301\n",
      "Epoch 725/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0267 - val_loss: 0.0297\n",
      "Epoch 726/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0264 - val_loss: 0.0300\n",
      "Epoch 727/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0255 - val_loss: 0.0260\n",
      "Epoch 728/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0242 - val_loss: 0.0258\n",
      "Epoch 729/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0240 - val_loss: 0.0262\n",
      "Epoch 730/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0235 - val_loss: 0.0245\n",
      "Epoch 731/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0235 - val_loss: 0.0242\n",
      "Epoch 732/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0233 - val_loss: 0.0254\n",
      "Epoch 733/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0239 - val_loss: 0.0259\n",
      "Epoch 734/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0238 - val_loss: 0.0264\n",
      "Epoch 735/1000\n",
      "3232/3232 [==============================] - 86s 26ms/sample - loss: 0.0228 - val_loss: 0.0238\n",
      "Epoch 736/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0239 - val_loss: 0.0240\n",
      "Epoch 737/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0233 - val_loss: 0.0240\n",
      "Epoch 738/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0245 - val_loss: 0.0241\n",
      "Epoch 739/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0240 - val_loss: 0.0252\n",
      "Epoch 740/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0226 - val_loss: 0.0259\n",
      "Epoch 741/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0237 - val_loss: 0.0252\n",
      "Epoch 742/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0234 - val_loss: 0.0260\n",
      "Epoch 743/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0241 - val_loss: 0.0238\n",
      "Epoch 744/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0240 - val_loss: 0.0238\n",
      "Epoch 745/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0225 - val_loss: 0.0245\n",
      "Epoch 746/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0234 - val_loss: 0.0242\n",
      "Epoch 747/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0228 - val_loss: 0.0247\n",
      "Epoch 748/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0250 - val_loss: 0.0250\n",
      "Epoch 749/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0231 - val_loss: 0.0237\n",
      "Epoch 750/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0235 - val_loss: 0.0242\n",
      "Epoch 751/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0231 - val_loss: 0.0251\n",
      "Epoch 752/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0239 - val_loss: 0.0246\n",
      "Epoch 753/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0235 - val_loss: 0.0257\n",
      "Epoch 754/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0238 - val_loss: 0.0244\n",
      "Epoch 755/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0233 - val_loss: 0.0240\n",
      "Epoch 756/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0224 - val_loss: 0.0239\n",
      "Epoch 757/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0223 - val_loss: 0.0263\n",
      "Epoch 758/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0234 - val_loss: 0.0245\n",
      "Epoch 759/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0222 - val_loss: 0.0261\n",
      "Epoch 760/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0232 - val_loss: 0.0245\n",
      "Epoch 761/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0227 - val_loss: 0.0259\n",
      "Epoch 762/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0234 - val_loss: 0.0243\n",
      "Epoch 763/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0238 - val_loss: 0.0246\n",
      "Epoch 764/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0232 - val_loss: 0.0249\n",
      "Epoch 765/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0225 - val_loss: 0.0255\n",
      "Epoch 766/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0244 - val_loss: 0.0279\n",
      "Epoch 767/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0230 - val_loss: 0.0249\n",
      "Epoch 768/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0240 - val_loss: 0.0257\n",
      "Epoch 769/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0229 - val_loss: 0.0254\n",
      "Epoch 770/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0232 - val_loss: 0.0290\n",
      "Epoch 771/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0235 - val_loss: 0.0244\n",
      "Epoch 772/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0227 - val_loss: 0.0246\n",
      "Epoch 773/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0252 - val_loss: 0.0271\n",
      "Epoch 774/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0242 - val_loss: 0.0243\n",
      "Epoch 775/1000\n",
      "3232/3232 [==============================] - 86s 26ms/sample - loss: 0.0224 - val_loss: 0.0240\n",
      "Epoch 776/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0228 - val_loss: 0.0243\n",
      "Epoch 777/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0239 - val_loss: 0.0284\n",
      "Epoch 778/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0253 - val_loss: 0.0242\n",
      "Epoch 779/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0222 - val_loss: 0.0262\n",
      "Epoch 780/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0229 - val_loss: 0.0255\n",
      "Epoch 781/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0224 - val_loss: 0.0249\n",
      "Epoch 782/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0228 - val_loss: 0.0237\n",
      "Epoch 783/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0232 - val_loss: 0.0239\n",
      "Epoch 784/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0230 - val_loss: 0.0254\n",
      "Epoch 785/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0232 - val_loss: 0.0329\n",
      "Epoch 786/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0251 - val_loss: 0.0288\n",
      "Epoch 787/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0240 - val_loss: 0.0280\n",
      "Epoch 788/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0243 - val_loss: 0.0250\n",
      "Epoch 789/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0241 - val_loss: 0.0249\n",
      "Epoch 790/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0245 - val_loss: 0.0246\n",
      "Epoch 791/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0229 - val_loss: 0.0263\n",
      "Epoch 792/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0236 - val_loss: 0.0249\n",
      "Epoch 793/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0236 - val_loss: 0.0247\n",
      "Epoch 794/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0223 - val_loss: 0.0259\n",
      "Epoch 795/1000\n",
      "3232/3232 [==============================] - 84s 26ms/sample - loss: 0.0229 - val_loss: 0.0248\n",
      "Epoch 796/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0244 - val_loss: 0.0239\n",
      "Epoch 797/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0232 - val_loss: 0.0256\n",
      "Epoch 798/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0235 - val_loss: 0.0260\n",
      "Epoch 799/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0224 - val_loss: 0.0251\n",
      "Epoch 800/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0223 - val_loss: 0.0259\n",
      "Epoch 801/1000\n",
      "3232/3232 [==============================] - 91s 28ms/sample - loss: 0.0239 - val_loss: 0.0281\n",
      "Epoch 802/1000\n",
      "3232/3232 [==============================] - 91s 28ms/sample - loss: 0.0229 - val_loss: 0.0253\n",
      "Epoch 803/1000\n",
      "3232/3232 [==============================] - 8820s 3s/sample - loss: 0.0242 - val_loss: 0.0252\n",
      "Epoch 804/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0231 - val_loss: 0.0240\n",
      "Epoch 805/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0235 - val_loss: 0.0252\n",
      "Epoch 806/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0234 - val_loss: 0.0250\n",
      "Epoch 807/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0226 - val_loss: 0.0283\n",
      "Epoch 808/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0244 - val_loss: 0.0247\n",
      "Epoch 809/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0222 - val_loss: 0.0241\n",
      "Epoch 810/1000\n",
      "3232/3232 [==============================] - 86s 26ms/sample - loss: 0.0242 - val_loss: 0.0270\n",
      "Epoch 811/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0253 - val_loss: 0.0257\n",
      "Epoch 812/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0231 - val_loss: 0.0251\n",
      "Epoch 813/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0227 - val_loss: 0.0250\n",
      "Epoch 814/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0223 - val_loss: 0.0247\n",
      "Epoch 815/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0241 - val_loss: 0.0266\n",
      "Epoch 816/1000\n",
      "3232/3232 [==============================] - 86s 26ms/sample - loss: 0.0249 - val_loss: 0.0274\n",
      "Epoch 817/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0234 - val_loss: 0.0255\n",
      "Epoch 818/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0260 - val_loss: 0.0260\n",
      "Epoch 819/1000\n",
      "3232/3232 [==============================] - 86s 27ms/sample - loss: 0.0267 - val_loss: 0.0283\n",
      "Epoch 820/1000\n",
      "3232/3232 [==============================] - 86s 26ms/sample - loss: 0.0258 - val_loss: 0.0258\n",
      "Epoch 821/1000\n",
      "3232/3232 [==============================] - 85s 26ms/sample - loss: 0.0252 - val_loss: 0.0255\n",
      "Epoch 822/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0257 - val_loss: 0.0274\n",
      "Epoch 823/1000\n",
      "3232/3232 [==============================] - 93s 29ms/sample - loss: 0.0243 - val_loss: 0.0268\n",
      "Epoch 824/1000\n",
      "3232/3232 [==============================] - 94s 29ms/sample - loss: 0.0245 - val_loss: 0.0251\n",
      "Epoch 825/1000\n",
      "3232/3232 [==============================] - 109s 34ms/sample - loss: 0.0238 - val_loss: 0.0272\n",
      "Epoch 826/1000\n",
      "3232/3232 [==============================] - 109s 34ms/sample - loss: 0.0247 - val_loss: 0.0266\n",
      "Epoch 827/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232/3232 [==============================] - 103s 32ms/sample - loss: 0.0240 - val_loss: 0.0273\n",
      "Epoch 828/1000\n",
      "3232/3232 [==============================] - 104s 32ms/sample - loss: 0.0236 - val_loss: 0.0259\n",
      "Epoch 829/1000\n",
      "3232/3232 [==============================] - 104s 32ms/sample - loss: 0.0232 - val_loss: 0.0250\n",
      "Epoch 830/1000\n",
      "3232/3232 [==============================] - 94s 29ms/sample - loss: 0.0226 - val_loss: 0.0254\n",
      "Epoch 831/1000\n",
      "3232/3232 [==============================] - 115s 36ms/sample - loss: 0.0229 - val_loss: 0.0258\n",
      "Epoch 832/1000\n",
      "3232/3232 [==============================] - 106s 33ms/sample - loss: 0.0239 - val_loss: 0.0238\n",
      "Epoch 833/1000\n",
      "3232/3232 [==============================] - 113s 35ms/sample - loss: 0.0241 - val_loss: 0.0274\n",
      "Epoch 834/1000\n",
      "3232/3232 [==============================] - 117s 36ms/sample - loss: 0.0256 - val_loss: 0.0258\n",
      "Epoch 835/1000\n",
      "3232/3232 [==============================] - 120s 37ms/sample - loss: 0.0232 - val_loss: 0.0246\n",
      "Epoch 836/1000\n",
      "3232/3232 [==============================] - 113s 35ms/sample - loss: 0.0231 - val_loss: 0.0258\n",
      "Epoch 837/1000\n",
      "3232/3232 [==============================] - 113s 35ms/sample - loss: 0.0232 - val_loss: 0.0247\n",
      "Epoch 838/1000\n",
      "3232/3232 [==============================] - 116s 36ms/sample - loss: 0.0226 - val_loss: 0.0257\n",
      "Epoch 839/1000\n",
      "3232/3232 [==============================] - 124s 38ms/sample - loss: 0.0238 - val_loss: 0.0241\n",
      "Epoch 840/1000\n",
      "3232/3232 [==============================] - 139s 43ms/sample - loss: 0.0239 - val_loss: 0.0253\n",
      "Epoch 841/1000\n",
      "3232/3232 [==============================] - 129s 40ms/sample - loss: 0.0228 - val_loss: 0.0250\n",
      "Epoch 842/1000\n",
      "3232/3232 [==============================] - 125s 39ms/sample - loss: 0.0230 - val_loss: 0.0250\n",
      "Epoch 843/1000\n",
      "3232/3232 [==============================] - 113s 35ms/sample - loss: 0.0231 - val_loss: 0.0250\n",
      "Epoch 844/1000\n",
      "3232/3232 [==============================] - 114s 35ms/sample - loss: 0.0215 - val_loss: 0.0259\n",
      "Epoch 845/1000\n",
      "3232/3232 [==============================] - 115s 36ms/sample - loss: 0.0231 - val_loss: 0.0241\n",
      "Epoch 846/1000\n",
      "3232/3232 [==============================] - 119s 37ms/sample - loss: 0.0237 - val_loss: 0.0259\n",
      "Epoch 847/1000\n",
      "3232/3232 [==============================] - 120s 37ms/sample - loss: 0.0231 - val_loss: 0.0255\n",
      "Epoch 848/1000\n",
      "3232/3232 [==============================] - 117s 36ms/sample - loss: 0.0229 - val_loss: 0.0265\n",
      "Epoch 849/1000\n",
      "3232/3232 [==============================] - 115s 36ms/sample - loss: 0.0224 - val_loss: 0.0241\n",
      "Epoch 850/1000\n",
      "3232/3232 [==============================] - 109s 34ms/sample - loss: 0.0233 - val_loss: 0.0237\n",
      "Epoch 851/1000\n",
      "3232/3232 [==============================] - 112s 35ms/sample - loss: 0.0231 - val_loss: 0.0241\n",
      "Epoch 852/1000\n",
      "3232/3232 [==============================] - 106s 33ms/sample - loss: 0.0227 - val_loss: 0.0253\n",
      "Epoch 853/1000\n",
      "3232/3232 [==============================] - 102s 31ms/sample - loss: 0.0230 - val_loss: 0.0250\n",
      "Epoch 854/1000\n",
      "3232/3232 [==============================] - 95s 29ms/sample - loss: 0.0223 - val_loss: 0.0262\n",
      "Epoch 855/1000\n",
      "3232/3232 [==============================] - 96s 30ms/sample - loss: 0.0238 - val_loss: 0.0239\n",
      "Epoch 856/1000\n",
      "3232/3232 [==============================] - 100s 31ms/sample - loss: 0.0229 - val_loss: 0.0260\n",
      "Epoch 857/1000\n",
      "3232/3232 [==============================] - 94s 29ms/sample - loss: 0.0231 - val_loss: 0.0268\n",
      "Epoch 858/1000\n",
      "3232/3232 [==============================] - 99s 31ms/sample - loss: 0.0240 - val_loss: 0.0258\n",
      "Epoch 859/1000\n",
      "3232/3232 [==============================] - 95s 29ms/sample - loss: 0.0240 - val_loss: 0.0239\n",
      "Epoch 860/1000\n",
      "3232/3232 [==============================] - 95s 29ms/sample - loss: 0.0229 - val_loss: 0.0254\n",
      "Epoch 861/1000\n",
      "3232/3232 [==============================] - 110s 34ms/sample - loss: 0.0239 - val_loss: 0.0248\n",
      "Epoch 862/1000\n",
      "3232/3232 [==============================] - 107s 33ms/sample - loss: 0.0232 - val_loss: 0.0248\n",
      "Epoch 863/1000\n",
      "3232/3232 [==============================] - 116s 36ms/sample - loss: 0.0228 - val_loss: 0.0260\n",
      "Epoch 864/1000\n",
      "3232/3232 [==============================] - 111s 34ms/sample - loss: 0.0231 - val_loss: 0.0270\n",
      "Epoch 865/1000\n",
      "3232/3232 [==============================] - 112s 35ms/sample - loss: 0.0232 - val_loss: 0.0234\n",
      "Epoch 866/1000\n",
      "3232/3232 [==============================] - 115s 35ms/sample - loss: 0.0227 - val_loss: 0.0252\n",
      "Epoch 867/1000\n",
      "3232/3232 [==============================] - 113s 35ms/sample - loss: 0.0229 - val_loss: 0.0249\n",
      "Epoch 868/1000\n",
      "3232/3232 [==============================] - 112s 35ms/sample - loss: 0.0225 - val_loss: 0.0254\n",
      "Epoch 869/1000\n",
      "3232/3232 [==============================] - 105s 33ms/sample - loss: 0.0223 - val_loss: 0.0258\n",
      "Epoch 870/1000\n",
      "3232/3232 [==============================] - 104s 32ms/sample - loss: 0.0226 - val_loss: 0.0251\n",
      "Epoch 871/1000\n",
      "3232/3232 [==============================] - 102s 32ms/sample - loss: 0.0229 - val_loss: 0.0255\n",
      "Epoch 872/1000\n",
      "3232/3232 [==============================] - 104s 32ms/sample - loss: 0.0234 - val_loss: 0.0262\n",
      "Epoch 873/1000\n",
      "3232/3232 [==============================] - 109s 34ms/sample - loss: 0.0232 - val_loss: 0.0264\n",
      "Epoch 874/1000\n",
      "3232/3232 [==============================] - 101s 31ms/sample - loss: 0.0233 - val_loss: 0.0249\n",
      "Epoch 875/1000\n",
      "3232/3232 [==============================] - 102s 32ms/sample - loss: 0.0234 - val_loss: 0.0235\n",
      "Epoch 876/1000\n",
      "3232/3232 [==============================] - 106s 33ms/sample - loss: 0.0226 - val_loss: 0.0240\n",
      "Epoch 877/1000\n",
      "3232/3232 [==============================] - 105s 33ms/sample - loss: 0.0234 - val_loss: 0.0244\n",
      "Epoch 878/1000\n",
      "3232/3232 [==============================] - 107s 33ms/sample - loss: 0.0236 - val_loss: 0.0250\n",
      "Epoch 879/1000\n",
      "3232/3232 [==============================] - 99s 31ms/sample - loss: 0.0238 - val_loss: 0.0264\n",
      "Epoch 880/1000\n",
      "3232/3232 [==============================] - 99s 31ms/sample - loss: 0.0232 - val_loss: 0.0243\n",
      "Epoch 881/1000\n",
      "3232/3232 [==============================] - 98s 30ms/sample - loss: 0.0230 - val_loss: 0.0251\n",
      "Epoch 882/1000\n",
      "3232/3232 [==============================] - 105s 32ms/sample - loss: 0.0247 - val_loss: 0.0248\n",
      "Epoch 883/1000\n",
      "3232/3232 [==============================] - 110s 34ms/sample - loss: 0.0221 - val_loss: 0.0256\n",
      "Epoch 884/1000\n",
      "3232/3232 [==============================] - 107s 33ms/sample - loss: 0.0236 - val_loss: 0.0247\n",
      "Epoch 885/1000\n",
      "3232/3232 [==============================] - 102s 32ms/sample - loss: 0.0234 - val_loss: 0.0245\n",
      "Epoch 886/1000\n",
      "3232/3232 [==============================] - 104s 32ms/sample - loss: 0.0230 - val_loss: 0.0244\n",
      "Epoch 887/1000\n",
      "3232/3232 [==============================] - 102s 32ms/sample - loss: 0.0221 - val_loss: 0.0254\n",
      "Epoch 888/1000\n",
      "3232/3232 [==============================] - 103s 32ms/sample - loss: 0.0233 - val_loss: 0.0258\n",
      "Epoch 889/1000\n",
      "3232/3232 [==============================] - 103s 32ms/sample - loss: 0.0238 - val_loss: 0.0252\n",
      "Epoch 890/1000\n",
      "3232/3232 [==============================] - 101s 31ms/sample - loss: 0.0252 - val_loss: 0.0246\n",
      "Epoch 891/1000\n",
      "3232/3232 [==============================] - 99s 31ms/sample - loss: 0.0236 - val_loss: 0.0253\n",
      "Epoch 892/1000\n",
      "3232/3232 [==============================] - 105s 32ms/sample - loss: 0.0240 - val_loss: 0.0248\n",
      "Epoch 893/1000\n",
      "3232/3232 [==============================] - 101s 31ms/sample - loss: 0.0239 - val_loss: 0.0250\n",
      "Epoch 894/1000\n",
      "3232/3232 [==============================] - 104s 32ms/sample - loss: 0.0223 - val_loss: 0.0243\n",
      "Epoch 895/1000\n",
      "3232/3232 [==============================] - 100s 31ms/sample - loss: 0.0237 - val_loss: 0.0248\n",
      "Epoch 896/1000\n",
      "3232/3232 [==============================] - 102s 32ms/sample - loss: 0.0228 - val_loss: 0.0260\n",
      "Epoch 897/1000\n",
      "3232/3232 [==============================] - 103s 32ms/sample - loss: 0.0231 - val_loss: 0.0251\n",
      "Epoch 898/1000\n",
      "3232/3232 [==============================] - 100s 31ms/sample - loss: 0.0224 - val_loss: 0.0258\n",
      "Epoch 899/1000\n",
      "3232/3232 [==============================] - 102s 32ms/sample - loss: 0.0232 - val_loss: 0.0260\n",
      "Epoch 900/1000\n",
      "3232/3232 [==============================] - 99s 31ms/sample - loss: 0.0235 - val_loss: 0.0266\n",
      "Epoch 901/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232/3232 [==============================] - 99s 31ms/sample - loss: 0.0232 - val_loss: 0.0249\n",
      "Epoch 902/1000\n",
      "3232/3232 [==============================] - 96s 30ms/sample - loss: 0.0229 - val_loss: 0.0257\n",
      "Epoch 903/1000\n",
      "3232/3232 [==============================] - 102s 31ms/sample - loss: 0.0220 - val_loss: 0.0266\n",
      "Epoch 904/1000\n",
      "3232/3232 [==============================] - 97s 30ms/sample - loss: 0.0225 - val_loss: 0.0281\n",
      "Epoch 905/1000\n",
      "3232/3232 [==============================] - 106s 33ms/sample - loss: 0.0247 - val_loss: 0.0245\n",
      "Epoch 906/1000\n",
      "3232/3232 [==============================] - 101s 31ms/sample - loss: 0.0228 - val_loss: 0.0241\n",
      "Epoch 907/1000\n",
      "3232/3232 [==============================] - 97s 30ms/sample - loss: 0.0231 - val_loss: 0.0241\n",
      "Epoch 908/1000\n",
      "3232/3232 [==============================] - 98s 30ms/sample - loss: 0.0236 - val_loss: 0.0249\n",
      "Epoch 909/1000\n",
      "3232/3232 [==============================] - 95s 29ms/sample - loss: 0.0231 - val_loss: 0.0254\n",
      "Epoch 910/1000\n",
      "3232/3232 [==============================] - 98s 30ms/sample - loss: 0.0217 - val_loss: 0.0254\n",
      "Epoch 911/1000\n",
      "3232/3232 [==============================] - 97s 30ms/sample - loss: 0.0222 - val_loss: 0.0252\n",
      "Epoch 912/1000\n",
      "3232/3232 [==============================] - 97s 30ms/sample - loss: 0.0232 - val_loss: 0.0277\n",
      "Epoch 913/1000\n",
      "3232/3232 [==============================] - 97s 30ms/sample - loss: 0.0225 - val_loss: 0.0249\n",
      "Epoch 914/1000\n",
      "3232/3232 [==============================] - 96s 30ms/sample - loss: 0.0228 - val_loss: 0.0253\n",
      "Epoch 915/1000\n",
      "3232/3232 [==============================] - 103s 32ms/sample - loss: 0.0233 - val_loss: 0.0246\n",
      "Epoch 916/1000\n",
      "3232/3232 [==============================] - 108s 33ms/sample - loss: 0.0236 - val_loss: 0.0248\n",
      "Epoch 917/1000\n",
      "3232/3232 [==============================] - 96s 30ms/sample - loss: 0.0222 - val_loss: 0.0252\n",
      "Epoch 918/1000\n",
      "3232/3232 [==============================] - 100s 31ms/sample - loss: 0.0228 - val_loss: 0.0246\n",
      "Epoch 919/1000\n",
      "3232/3232 [==============================] - 96s 30ms/sample - loss: 0.0236 - val_loss: 0.0257\n",
      "Epoch 920/1000\n",
      "3232/3232 [==============================] - 93s 29ms/sample - loss: 0.0226 - val_loss: 0.0243\n",
      "Epoch 921/1000\n",
      "3232/3232 [==============================] - 104s 32ms/sample - loss: 0.0235 - val_loss: 0.0244\n",
      "Epoch 922/1000\n",
      "3232/3232 [==============================] - 112s 35ms/sample - loss: 0.0222 - val_loss: 0.0246\n",
      "Epoch 923/1000\n",
      "3232/3232 [==============================] - 109s 34ms/sample - loss: 0.0224 - val_loss: 0.0250\n",
      "Epoch 924/1000\n",
      "3232/3232 [==============================] - 113s 35ms/sample - loss: 0.0224 - val_loss: 0.0242\n",
      "Epoch 925/1000\n",
      "3232/3232 [==============================] - 110s 34ms/sample - loss: 0.0229 - val_loss: 0.0254\n",
      "Epoch 926/1000\n",
      "3232/3232 [==============================] - 96s 30ms/sample - loss: 0.0225 - val_loss: 0.0246\n",
      "Epoch 927/1000\n",
      "3232/3232 [==============================] - 91s 28ms/sample - loss: 0.0219 - val_loss: 0.0235\n",
      "Epoch 928/1000\n",
      "3232/3232 [==============================] - 90s 28ms/sample - loss: 0.0219 - val_loss: 0.0250\n",
      "Epoch 929/1000\n",
      "3232/3232 [==============================] - 90s 28ms/sample - loss: 0.0226 - val_loss: 0.0249\n",
      "Epoch 930/1000\n",
      "3232/3232 [==============================] - 90s 28ms/sample - loss: 0.0226 - val_loss: 0.0255\n",
      "Epoch 931/1000\n",
      "3232/3232 [==============================] - 90s 28ms/sample - loss: 0.0221 - val_loss: 0.0241\n",
      "Epoch 932/1000\n",
      "3232/3232 [==============================] - 89s 27ms/sample - loss: 0.0236 - val_loss: 0.0249\n",
      "Epoch 933/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0229 - val_loss: 0.0261\n",
      "Epoch 934/1000\n",
      "3232/3232 [==============================] - 89s 27ms/sample - loss: 0.0232 - val_loss: 0.0245\n",
      "Epoch 935/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0235 - val_loss: 0.0253\n",
      "Epoch 936/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0230 - val_loss: 0.0251\n",
      "Epoch 937/1000\n",
      "3232/3232 [==============================] - 89s 27ms/sample - loss: 0.0224 - val_loss: 0.0249\n",
      "Epoch 938/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0229 - val_loss: 0.0257\n",
      "Epoch 939/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0226 - val_loss: 0.0253\n",
      "Epoch 940/1000\n",
      "3232/3232 [==============================] - 89s 27ms/sample - loss: 0.0225 - val_loss: 0.0247\n",
      "Epoch 941/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0242 - val_loss: 0.0239\n",
      "Epoch 942/1000\n",
      "3232/3232 [==============================] - 89s 27ms/sample - loss: 0.0223 - val_loss: 0.0260\n",
      "Epoch 943/1000\n",
      "3232/3232 [==============================] - 89s 27ms/sample - loss: 0.0221 - val_loss: 0.0257\n",
      "Epoch 944/1000\n",
      "3232/3232 [==============================] - 89s 27ms/sample - loss: 0.0222 - val_loss: 0.0239\n",
      "Epoch 945/1000\n",
      "3232/3232 [==============================] - 89s 27ms/sample - loss: 0.0229 - val_loss: 0.0246\n",
      "Epoch 946/1000\n",
      "3232/3232 [==============================] - 90s 28ms/sample - loss: 0.0231 - val_loss: 0.0246\n",
      "Epoch 947/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0236 - val_loss: 0.0237\n",
      "Epoch 948/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0228 - val_loss: 0.0242\n",
      "Epoch 949/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0226 - val_loss: 0.0245\n",
      "Epoch 950/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0230 - val_loss: 0.0272\n",
      "Epoch 951/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0232 - val_loss: 0.0247\n",
      "Epoch 952/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0219 - val_loss: 0.0258\n",
      "Epoch 953/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0217 - val_loss: 0.0241\n",
      "Epoch 954/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0233 - val_loss: 0.0237\n",
      "Epoch 955/1000\n",
      "3232/3232 [==============================] - 89s 27ms/sample - loss: 0.0236 - val_loss: 0.0241\n",
      "Epoch 956/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0223 - val_loss: 0.0243\n",
      "Epoch 957/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0236 - val_loss: 0.0245\n",
      "Epoch 958/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0227 - val_loss: 0.0244\n",
      "Epoch 959/1000\n",
      "3232/3232 [==============================] - 90s 28ms/sample - loss: 0.0228 - val_loss: 0.0251\n",
      "Epoch 960/1000\n",
      "3232/3232 [==============================] - 90s 28ms/sample - loss: 0.0230 - val_loss: 0.0264\n",
      "Epoch 961/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0242 - val_loss: 0.0244\n",
      "Epoch 962/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0233 - val_loss: 0.0248\n",
      "Epoch 963/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0232 - val_loss: 0.0237\n",
      "Epoch 964/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0238 - val_loss: 0.0234\n",
      "Epoch 965/1000\n",
      "3232/3232 [==============================] - 90s 28ms/sample - loss: 0.0225 - val_loss: 0.0244\n",
      "Epoch 966/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0236 - val_loss: 0.0243\n",
      "Epoch 967/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0229 - val_loss: 0.0258\n",
      "Epoch 968/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0224 - val_loss: 0.0236\n",
      "Epoch 969/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0218 - val_loss: 0.0251\n",
      "Epoch 970/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0229 - val_loss: 0.0274\n",
      "Epoch 971/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0246 - val_loss: 0.0243\n",
      "Epoch 972/1000\n",
      "3232/3232 [==============================] - 89s 28ms/sample - loss: 0.0217 - val_loss: 0.0261\n",
      "Epoch 973/1000\n",
      "3232/3232 [==============================] - 89s 27ms/sample - loss: 0.0217 - val_loss: 0.0255\n",
      "Epoch 974/1000\n",
      "3232/3232 [==============================] - 90s 28ms/sample - loss: 0.0241 - val_loss: 0.0244\n",
      "Epoch 975/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0229 - val_loss: 0.0274\n",
      "Epoch 976/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0225 - val_loss: 0.0250\n",
      "Epoch 977/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0230 - val_loss: 0.0235\n",
      "Epoch 978/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0225 - val_loss: 0.0241\n",
      "Epoch 979/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0238 - val_loss: 0.0255\n",
      "Epoch 980/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0232 - val_loss: 0.0286\n",
      "Epoch 981/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0226 - val_loss: 0.0246\n",
      "Epoch 982/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0220 - val_loss: 0.0237\n",
      "Epoch 983/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0219 - val_loss: 0.0251\n",
      "Epoch 984/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0230 - val_loss: 0.0240\n",
      "Epoch 985/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0234 - val_loss: 0.0244\n",
      "Epoch 986/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0227 - val_loss: 0.0245\n",
      "Epoch 987/1000\n",
      "3232/3232 [==============================] - 89s 27ms/sample - loss: 0.0233 - val_loss: 0.0242\n",
      "Epoch 988/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0225 - val_loss: 0.0263\n",
      "Epoch 989/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0241 - val_loss: 0.0255\n",
      "Epoch 990/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0231 - val_loss: 0.0248\n",
      "Epoch 991/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0221 - val_loss: 0.0256\n",
      "Epoch 992/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0242 - val_loss: 0.0300\n",
      "Epoch 993/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0268 - val_loss: 0.0294\n",
      "Epoch 994/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0265 - val_loss: 0.0291\n",
      "Epoch 995/1000\n",
      "3232/3232 [==============================] - 88s 27ms/sample - loss: 0.0250 - val_loss: 0.0280\n",
      "Epoch 996/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0254 - val_loss: 0.0277\n",
      "Epoch 997/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0258 - val_loss: 0.0270\n",
      "Epoch 998/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0268 - val_loss: 0.0268\n",
      "Epoch 999/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0242 - val_loss: 0.0259\n",
      "Epoch 1000/1000\n",
      "3232/3232 [==============================] - 87s 27ms/sample - loss: 0.0229 - val_loss: 0.0260\n",
      "Predicting...\n",
      "Reshaping predicted [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicting shape (3233,)\n",
      "Training duration (s) :  92837.63885736465\n"
     ]
    }
   ],
   "source": [
    "#LSTM  NETWORK\n",
    "\"\"\" Inspired by example from\n",
    "https://github.com/Vict0rSch/deep_learning/tree/master/keras/recurrent\n",
    "Uses the TensorFlow backend\n",
    "The basic idea is to detect anomalies in a time-series.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Global hyper-parameters\n",
    "epochs = 1000\n",
    "batch_size = 1 #always 1\n",
    "\n",
    "global_start_time = time.time()\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape) \n",
    "\n",
    "\n",
    "print( '\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "model = build_model()\n",
    "print(\"Buildibg Model...\")\n",
    "\n",
    "try:\n",
    "    print(\"Training...\")\n",
    "    history = model.fit( X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test,y_test))\n",
    "    print(\"Predicting...\")\n",
    "    predicted = model.predict(X_test)\n",
    "    \n",
    "    threshold =0.328\n",
    "    for i in range(predicted.shape[0]): \n",
    "        if predicted[i] > threshold:\n",
    "            predicted[i] = 1\n",
    "        else:\n",
    "            predicted[i] = 0\n",
    "    print(\"Reshaping predicted\", predicted)\n",
    "    \n",
    "    predicted = np.reshape(predicted, (predicted.size,))\n",
    "    print(\"Predicting shape\", predicted.shape)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"prediction exception\")\n",
    "    print ('Training duration (s) : ', time.time() - global_start_time)\n",
    "\n",
    "\n",
    "\n",
    "print( 'Training duration (s) : ', time.time() - global_start_time)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#plt.plot(.history['loss'])\n",
    "##plt.plot(history.history['accuracy'])\n",
    "#plt.title('model train vs validation loss')\n",
    "#plt.ylabel('loss')\n",
    "#plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'validation'], loc='upper right')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.multiclass import unique_labels\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[0 1]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_test2 = y_test.astype(int)\n",
    "predicted2 = predicted.round().astype(int)\n",
    "#print(y_test.astype(int))\n",
    "print(unique_labels(y_test2))\n",
    "print(unique_labels(predicted2))\n",
    "\n",
    "print(predicted.round())\n",
    "\n",
    "numpy.savetxt(\"predicted.csv\", predicted, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[3035   75]\n",
      " [  65   58]]\n",
      "Normalized confusion matrix\n",
      "[[0.98 0.02]\n",
      " [0.53 0.47]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "class_names = [0, 1]#y_test.target_names\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test2, predicted2, classes=unique_labels(y_test2, predicted2), title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test2, predicted2, classes=unique_labels(y_test2, predicted2), normalize=True, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro\n",
      "Precision Score 0.70756\n",
      "Recall Score 0.72371\n",
      "F1 Score 0.71529\n"
     ]
    }
   ],
   "source": [
    "print(\"Macro\")\n",
    "print(\"Precision Score\",round(metrics.precision_score(y_test, predicted.round(),average='macro'), ndigits=5))\n",
    "print(\"Recall Score\",round(metrics.recall_score(y_test, predicted.round(),average='macro'), ndigits=5))\n",
    "print(\"F1 Score\",round(metrics.f1_score(y_test, predicted.round(),average='macro'), ndigits=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
